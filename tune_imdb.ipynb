{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import DistilBertModel, DistilBertForMaskedLM, DistilBertTokenizer\n",
    "from optimum.quanto import freeze, quantize, qint8\n",
    "import datasets\n",
    "from transformers import TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"distilbert-base-uncased\"\n",
    "#model = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-uncased\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"imdb\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    tokens = tokenizer(data[\"text\"], truncation=True, padding = 'max_length',  max_length=512)\n",
    "    tokens[\"label\"] = data[\"label\"]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dataset.map(preprocess, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokens['train'].features['label'].names\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = {}, {}\n",
    "\n",
    "for idx, lbl in enumerate(labels):\n",
    "    label2id[lbl] = idx\n",
    "    id2label[idx] = lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokens[\"train\"].shuffle(seed=11).select(range(2000))\n",
    "small_eval_dataset = tokens[\"train\"].shuffle(seed=11).select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AutoModelForSequenceClassification, DistilBertConfig, DataCollatorWithPadding\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels = num_labels,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Preprocess the logits to ensure they are in the correct format for metric computation.\n",
    "    This function will be called during the evaluation process.\n",
    "    \"\"\"\n",
    "    if isinstance(logits, tuple):  \n",
    "        logits = logits[0]  # get logit tensors\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    return pred_ids, labels\n",
    "    \n",
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    return accuracy.compute(predictions=predictions[0], references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21768/3985490060.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.00005\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './imdb_tune_distilbert',\n",
    "    num_train_epochs = EPOCHS,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    logging_dir = './logs',\n",
    "    load_best_model_at_end= True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps = 500,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=['tensorboard'],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=small_train_dataset,         \n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics = compute_metrics,\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 33:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.182836</td>\n",
       "      <td>0.941500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.39196124267578125, metrics={'train_runtime': 2017.9639, 'train_samples_per_second': 0.991, 'train_steps_per_second': 0.062, 'total_flos': 264934797312000.0, 'train_loss': 0.39196124267578125, 'epoch': 1.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/se/coding/school/exjobb/mycode/.venv/lib/python3.12/site-packages/transformers/training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 02:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after quantization: 0.48833333333333334\n"
     ]
    }
   ],
   "source": [
    "untrained = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels = num_labels,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  \n",
    "    per_device_eval_batch_size=2, \n",
    "    use_cpu=True,  \n",
    "    no_cuda = True,\n",
    "    logging_dir=\"./logs\",  \n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=untrained,\n",
    "    args=training_args,\n",
    "    eval_dataset=small_eval_dataset.select(range(600)),\n",
    "    data_collator = data_collator,\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"accuracy no training: \", trainer.evaluate())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
