{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "#from transformers import pipeline\n",
    "from transformers import DistilBertModel, DistilBertForMaskedLM, DistilBertTokenizer, IBertForSequenceClassification, AutoTokenizer\n",
    "#from optimum.quanto import freeze, quantize, qint8\n",
    "import datasets\n",
    "from transformers import TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"kssteven/ibert-roberta-base\"\n",
    "#model = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-uncased\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kssteven/ibert-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"imdb\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    tokens = tokenizer(data[\"text\"], truncation=True, padding = 'max_length',  max_length=512)\n",
    "    tokens[\"label\"] = data[\"label\"]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:07<00:00, 3327.30 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:15<00:00, 3150.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokens = dataset.map(preprocess, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokens['train'].features['label'].names\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = {}, {}\n",
    "\n",
    "for idx, lbl in enumerate(labels):\n",
    "    label2id[lbl] = idx\n",
    "    id2label[idx] = lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokens[\"train\"].shuffle(seed=11).select(range(2000))\n",
    "small_eval_dataset = tokens[\"train\"].shuffle(seed=11).select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of IBertForSequenceClassification were not initialized from the model checkpoint at kssteven/ibert-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'ibert.embeddings.LayerNorm.activation.act_scaling_factor', 'ibert.embeddings.LayerNorm.activation.x_max', 'ibert.embeddings.LayerNorm.activation.x_min', 'ibert.embeddings.LayerNorm.shift', 'ibert.embeddings.embeddings_act1.act_scaling_factor', 'ibert.embeddings.embeddings_act1.x_max', 'ibert.embeddings.embeddings_act1.x_min', 'ibert.embeddings.embeddings_act2.act_scaling_factor', 'ibert.embeddings.embeddings_act2.x_max', 'ibert.embeddings.embeddings_act2.x_min', 'ibert.embeddings.output_activation.act_scaling_factor', 'ibert.embeddings.output_activation.x_max', 'ibert.embeddings.output_activation.x_min', 'ibert.embeddings.position_embeddings.weight_integer', 'ibert.embeddings.position_embeddings.weight_scaling_factor', 'ibert.embeddings.token_type_embeddings.weight_integer', 'ibert.embeddings.token_type_embeddings.weight_scaling_factor', 'ibert.embeddings.word_embeddings.weight_integer', 'ibert.embeddings.word_embeddings.weight_scaling_factor', 'ibert.encoder.layer.0.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.0.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.0.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.0.attention.output.LayerNorm.shift', 'ibert.encoder.layer.0.attention.output.dense.bias_integer', 'ibert.encoder.layer.0.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.0.attention.output.dense.weight_integer', 'ibert.encoder.layer.0.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.0.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.0.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.0.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.0.attention.output.output_activation.x_max', 'ibert.encoder.layer.0.attention.output.output_activation.x_min', 'ibert.encoder.layer.0.attention.self.key.bias_integer', 'ibert.encoder.layer.0.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.0.attention.self.key.weight_integer', 'ibert.encoder.layer.0.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.0.attention.self.key_activation.x_max', 'ibert.encoder.layer.0.attention.self.key_activation.x_min', 'ibert.encoder.layer.0.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.0.attention.self.output_activation.x_max', 'ibert.encoder.layer.0.attention.self.output_activation.x_min', 'ibert.encoder.layer.0.attention.self.query.bias_integer', 'ibert.encoder.layer.0.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.0.attention.self.query.weight_integer', 'ibert.encoder.layer.0.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.0.attention.self.query_activation.x_max', 'ibert.encoder.layer.0.attention.self.query_activation.x_min', 'ibert.encoder.layer.0.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.0.attention.self.softmax.act.x_max', 'ibert.encoder.layer.0.attention.self.softmax.act.x_min', 'ibert.encoder.layer.0.attention.self.value.bias_integer', 'ibert.encoder.layer.0.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.0.attention.self.value.weight_integer', 'ibert.encoder.layer.0.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.0.attention.self.value_activation.x_max', 'ibert.encoder.layer.0.attention.self.value_activation.x_min', 'ibert.encoder.layer.0.intermediate.dense.bias_integer', 'ibert.encoder.layer.0.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.0.intermediate.dense.weight_integer', 'ibert.encoder.layer.0.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.0.intermediate.output_activation.x_max', 'ibert.encoder.layer.0.intermediate.output_activation.x_min', 'ibert.encoder.layer.0.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.0.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.0.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.0.output.LayerNorm.shift', 'ibert.encoder.layer.0.output.dense.bias_integer', 'ibert.encoder.layer.0.output.dense.fc_scaling_factor', 'ibert.encoder.layer.0.output.dense.weight_integer', 'ibert.encoder.layer.0.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.0.output.ln_input_act.x_max', 'ibert.encoder.layer.0.output.ln_input_act.x_min', 'ibert.encoder.layer.0.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.0.output.output_activation.x_max', 'ibert.encoder.layer.0.output.output_activation.x_min', 'ibert.encoder.layer.0.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.0.pre_intermediate_act.x_max', 'ibert.encoder.layer.0.pre_intermediate_act.x_min', 'ibert.encoder.layer.0.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.0.pre_output_act.x_max', 'ibert.encoder.layer.0.pre_output_act.x_min', 'ibert.encoder.layer.1.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.1.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.1.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.1.attention.output.LayerNorm.shift', 'ibert.encoder.layer.1.attention.output.dense.bias_integer', 'ibert.encoder.layer.1.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.1.attention.output.dense.weight_integer', 'ibert.encoder.layer.1.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.1.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.1.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.1.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.1.attention.output.output_activation.x_max', 'ibert.encoder.layer.1.attention.output.output_activation.x_min', 'ibert.encoder.layer.1.attention.self.key.bias_integer', 'ibert.encoder.layer.1.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.1.attention.self.key.weight_integer', 'ibert.encoder.layer.1.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.1.attention.self.key_activation.x_max', 'ibert.encoder.layer.1.attention.self.key_activation.x_min', 'ibert.encoder.layer.1.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.1.attention.self.output_activation.x_max', 'ibert.encoder.layer.1.attention.self.output_activation.x_min', 'ibert.encoder.layer.1.attention.self.query.bias_integer', 'ibert.encoder.layer.1.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.1.attention.self.query.weight_integer', 'ibert.encoder.layer.1.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.1.attention.self.query_activation.x_max', 'ibert.encoder.layer.1.attention.self.query_activation.x_min', 'ibert.encoder.layer.1.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.1.attention.self.softmax.act.x_max', 'ibert.encoder.layer.1.attention.self.softmax.act.x_min', 'ibert.encoder.layer.1.attention.self.value.bias_integer', 'ibert.encoder.layer.1.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.1.attention.self.value.weight_integer', 'ibert.encoder.layer.1.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.1.attention.self.value_activation.x_max', 'ibert.encoder.layer.1.attention.self.value_activation.x_min', 'ibert.encoder.layer.1.intermediate.dense.bias_integer', 'ibert.encoder.layer.1.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.1.intermediate.dense.weight_integer', 'ibert.encoder.layer.1.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.1.intermediate.output_activation.x_max', 'ibert.encoder.layer.1.intermediate.output_activation.x_min', 'ibert.encoder.layer.1.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.1.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.1.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.1.output.LayerNorm.shift', 'ibert.encoder.layer.1.output.dense.bias_integer', 'ibert.encoder.layer.1.output.dense.fc_scaling_factor', 'ibert.encoder.layer.1.output.dense.weight_integer', 'ibert.encoder.layer.1.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.1.output.ln_input_act.x_max', 'ibert.encoder.layer.1.output.ln_input_act.x_min', 'ibert.encoder.layer.1.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.1.output.output_activation.x_max', 'ibert.encoder.layer.1.output.output_activation.x_min', 'ibert.encoder.layer.1.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.1.pre_intermediate_act.x_max', 'ibert.encoder.layer.1.pre_intermediate_act.x_min', 'ibert.encoder.layer.1.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.1.pre_output_act.x_max', 'ibert.encoder.layer.1.pre_output_act.x_min', 'ibert.encoder.layer.10.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.10.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.10.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.10.attention.output.LayerNorm.shift', 'ibert.encoder.layer.10.attention.output.dense.bias_integer', 'ibert.encoder.layer.10.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.10.attention.output.dense.weight_integer', 'ibert.encoder.layer.10.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.10.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.10.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.10.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.10.attention.output.output_activation.x_max', 'ibert.encoder.layer.10.attention.output.output_activation.x_min', 'ibert.encoder.layer.10.attention.self.key.bias_integer', 'ibert.encoder.layer.10.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.10.attention.self.key.weight_integer', 'ibert.encoder.layer.10.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.10.attention.self.key_activation.x_max', 'ibert.encoder.layer.10.attention.self.key_activation.x_min', 'ibert.encoder.layer.10.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.10.attention.self.output_activation.x_max', 'ibert.encoder.layer.10.attention.self.output_activation.x_min', 'ibert.encoder.layer.10.attention.self.query.bias_integer', 'ibert.encoder.layer.10.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.10.attention.self.query.weight_integer', 'ibert.encoder.layer.10.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.10.attention.self.query_activation.x_max', 'ibert.encoder.layer.10.attention.self.query_activation.x_min', 'ibert.encoder.layer.10.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.10.attention.self.softmax.act.x_max', 'ibert.encoder.layer.10.attention.self.softmax.act.x_min', 'ibert.encoder.layer.10.attention.self.value.bias_integer', 'ibert.encoder.layer.10.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.10.attention.self.value.weight_integer', 'ibert.encoder.layer.10.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.10.attention.self.value_activation.x_max', 'ibert.encoder.layer.10.attention.self.value_activation.x_min', 'ibert.encoder.layer.10.intermediate.dense.bias_integer', 'ibert.encoder.layer.10.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.10.intermediate.dense.weight_integer', 'ibert.encoder.layer.10.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.10.intermediate.output_activation.x_max', 'ibert.encoder.layer.10.intermediate.output_activation.x_min', 'ibert.encoder.layer.10.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.10.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.10.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.10.output.LayerNorm.shift', 'ibert.encoder.layer.10.output.dense.bias_integer', 'ibert.encoder.layer.10.output.dense.fc_scaling_factor', 'ibert.encoder.layer.10.output.dense.weight_integer', 'ibert.encoder.layer.10.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.10.output.ln_input_act.x_max', 'ibert.encoder.layer.10.output.ln_input_act.x_min', 'ibert.encoder.layer.10.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.10.output.output_activation.x_max', 'ibert.encoder.layer.10.output.output_activation.x_min', 'ibert.encoder.layer.10.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.10.pre_intermediate_act.x_max', 'ibert.encoder.layer.10.pre_intermediate_act.x_min', 'ibert.encoder.layer.10.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.10.pre_output_act.x_max', 'ibert.encoder.layer.10.pre_output_act.x_min', 'ibert.encoder.layer.11.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.11.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.11.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.11.attention.output.LayerNorm.shift', 'ibert.encoder.layer.11.attention.output.dense.bias_integer', 'ibert.encoder.layer.11.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.11.attention.output.dense.weight_integer', 'ibert.encoder.layer.11.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.11.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.11.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.11.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.11.attention.output.output_activation.x_max', 'ibert.encoder.layer.11.attention.output.output_activation.x_min', 'ibert.encoder.layer.11.attention.self.key.bias_integer', 'ibert.encoder.layer.11.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.11.attention.self.key.weight_integer', 'ibert.encoder.layer.11.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.11.attention.self.key_activation.x_max', 'ibert.encoder.layer.11.attention.self.key_activation.x_min', 'ibert.encoder.layer.11.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.11.attention.self.output_activation.x_max', 'ibert.encoder.layer.11.attention.self.output_activation.x_min', 'ibert.encoder.layer.11.attention.self.query.bias_integer', 'ibert.encoder.layer.11.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.11.attention.self.query.weight_integer', 'ibert.encoder.layer.11.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.11.attention.self.query_activation.x_max', 'ibert.encoder.layer.11.attention.self.query_activation.x_min', 'ibert.encoder.layer.11.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.11.attention.self.softmax.act.x_max', 'ibert.encoder.layer.11.attention.self.softmax.act.x_min', 'ibert.encoder.layer.11.attention.self.value.bias_integer', 'ibert.encoder.layer.11.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.11.attention.self.value.weight_integer', 'ibert.encoder.layer.11.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.11.attention.self.value_activation.x_max', 'ibert.encoder.layer.11.attention.self.value_activation.x_min', 'ibert.encoder.layer.11.intermediate.dense.bias_integer', 'ibert.encoder.layer.11.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.11.intermediate.dense.weight_integer', 'ibert.encoder.layer.11.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.11.intermediate.output_activation.x_max', 'ibert.encoder.layer.11.intermediate.output_activation.x_min', 'ibert.encoder.layer.11.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.11.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.11.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.11.output.LayerNorm.shift', 'ibert.encoder.layer.11.output.dense.bias_integer', 'ibert.encoder.layer.11.output.dense.fc_scaling_factor', 'ibert.encoder.layer.11.output.dense.weight_integer', 'ibert.encoder.layer.11.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.11.output.ln_input_act.x_max', 'ibert.encoder.layer.11.output.ln_input_act.x_min', 'ibert.encoder.layer.11.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.11.output.output_activation.x_max', 'ibert.encoder.layer.11.output.output_activation.x_min', 'ibert.encoder.layer.11.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.11.pre_intermediate_act.x_max', 'ibert.encoder.layer.11.pre_intermediate_act.x_min', 'ibert.encoder.layer.11.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.11.pre_output_act.x_max', 'ibert.encoder.layer.11.pre_output_act.x_min', 'ibert.encoder.layer.2.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.2.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.2.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.2.attention.output.LayerNorm.shift', 'ibert.encoder.layer.2.attention.output.dense.bias_integer', 'ibert.encoder.layer.2.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.2.attention.output.dense.weight_integer', 'ibert.encoder.layer.2.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.2.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.2.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.2.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.2.attention.output.output_activation.x_max', 'ibert.encoder.layer.2.attention.output.output_activation.x_min', 'ibert.encoder.layer.2.attention.self.key.bias_integer', 'ibert.encoder.layer.2.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.2.attention.self.key.weight_integer', 'ibert.encoder.layer.2.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.2.attention.self.key_activation.x_max', 'ibert.encoder.layer.2.attention.self.key_activation.x_min', 'ibert.encoder.layer.2.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.2.attention.self.output_activation.x_max', 'ibert.encoder.layer.2.attention.self.output_activation.x_min', 'ibert.encoder.layer.2.attention.self.query.bias_integer', 'ibert.encoder.layer.2.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.2.attention.self.query.weight_integer', 'ibert.encoder.layer.2.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.2.attention.self.query_activation.x_max', 'ibert.encoder.layer.2.attention.self.query_activation.x_min', 'ibert.encoder.layer.2.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.2.attention.self.softmax.act.x_max', 'ibert.encoder.layer.2.attention.self.softmax.act.x_min', 'ibert.encoder.layer.2.attention.self.value.bias_integer', 'ibert.encoder.layer.2.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.2.attention.self.value.weight_integer', 'ibert.encoder.layer.2.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.2.attention.self.value_activation.x_max', 'ibert.encoder.layer.2.attention.self.value_activation.x_min', 'ibert.encoder.layer.2.intermediate.dense.bias_integer', 'ibert.encoder.layer.2.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.2.intermediate.dense.weight_integer', 'ibert.encoder.layer.2.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.2.intermediate.output_activation.x_max', 'ibert.encoder.layer.2.intermediate.output_activation.x_min', 'ibert.encoder.layer.2.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.2.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.2.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.2.output.LayerNorm.shift', 'ibert.encoder.layer.2.output.dense.bias_integer', 'ibert.encoder.layer.2.output.dense.fc_scaling_factor', 'ibert.encoder.layer.2.output.dense.weight_integer', 'ibert.encoder.layer.2.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.2.output.ln_input_act.x_max', 'ibert.encoder.layer.2.output.ln_input_act.x_min', 'ibert.encoder.layer.2.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.2.output.output_activation.x_max', 'ibert.encoder.layer.2.output.output_activation.x_min', 'ibert.encoder.layer.2.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.2.pre_intermediate_act.x_max', 'ibert.encoder.layer.2.pre_intermediate_act.x_min', 'ibert.encoder.layer.2.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.2.pre_output_act.x_max', 'ibert.encoder.layer.2.pre_output_act.x_min', 'ibert.encoder.layer.3.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.3.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.3.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.3.attention.output.LayerNorm.shift', 'ibert.encoder.layer.3.attention.output.dense.bias_integer', 'ibert.encoder.layer.3.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.3.attention.output.dense.weight_integer', 'ibert.encoder.layer.3.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.3.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.3.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.3.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.3.attention.output.output_activation.x_max', 'ibert.encoder.layer.3.attention.output.output_activation.x_min', 'ibert.encoder.layer.3.attention.self.key.bias_integer', 'ibert.encoder.layer.3.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.3.attention.self.key.weight_integer', 'ibert.encoder.layer.3.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.3.attention.self.key_activation.x_max', 'ibert.encoder.layer.3.attention.self.key_activation.x_min', 'ibert.encoder.layer.3.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.3.attention.self.output_activation.x_max', 'ibert.encoder.layer.3.attention.self.output_activation.x_min', 'ibert.encoder.layer.3.attention.self.query.bias_integer', 'ibert.encoder.layer.3.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.3.attention.self.query.weight_integer', 'ibert.encoder.layer.3.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.3.attention.self.query_activation.x_max', 'ibert.encoder.layer.3.attention.self.query_activation.x_min', 'ibert.encoder.layer.3.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.3.attention.self.softmax.act.x_max', 'ibert.encoder.layer.3.attention.self.softmax.act.x_min', 'ibert.encoder.layer.3.attention.self.value.bias_integer', 'ibert.encoder.layer.3.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.3.attention.self.value.weight_integer', 'ibert.encoder.layer.3.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.3.attention.self.value_activation.x_max', 'ibert.encoder.layer.3.attention.self.value_activation.x_min', 'ibert.encoder.layer.3.intermediate.dense.bias_integer', 'ibert.encoder.layer.3.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.3.intermediate.dense.weight_integer', 'ibert.encoder.layer.3.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.3.intermediate.output_activation.x_max', 'ibert.encoder.layer.3.intermediate.output_activation.x_min', 'ibert.encoder.layer.3.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.3.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.3.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.3.output.LayerNorm.shift', 'ibert.encoder.layer.3.output.dense.bias_integer', 'ibert.encoder.layer.3.output.dense.fc_scaling_factor', 'ibert.encoder.layer.3.output.dense.weight_integer', 'ibert.encoder.layer.3.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.3.output.ln_input_act.x_max', 'ibert.encoder.layer.3.output.ln_input_act.x_min', 'ibert.encoder.layer.3.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.3.output.output_activation.x_max', 'ibert.encoder.layer.3.output.output_activation.x_min', 'ibert.encoder.layer.3.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.3.pre_intermediate_act.x_max', 'ibert.encoder.layer.3.pre_intermediate_act.x_min', 'ibert.encoder.layer.3.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.3.pre_output_act.x_max', 'ibert.encoder.layer.3.pre_output_act.x_min', 'ibert.encoder.layer.4.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.4.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.4.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.4.attention.output.LayerNorm.shift', 'ibert.encoder.layer.4.attention.output.dense.bias_integer', 'ibert.encoder.layer.4.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.4.attention.output.dense.weight_integer', 'ibert.encoder.layer.4.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.4.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.4.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.4.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.4.attention.output.output_activation.x_max', 'ibert.encoder.layer.4.attention.output.output_activation.x_min', 'ibert.encoder.layer.4.attention.self.key.bias_integer', 'ibert.encoder.layer.4.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.4.attention.self.key.weight_integer', 'ibert.encoder.layer.4.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.4.attention.self.key_activation.x_max', 'ibert.encoder.layer.4.attention.self.key_activation.x_min', 'ibert.encoder.layer.4.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.4.attention.self.output_activation.x_max', 'ibert.encoder.layer.4.attention.self.output_activation.x_min', 'ibert.encoder.layer.4.attention.self.query.bias_integer', 'ibert.encoder.layer.4.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.4.attention.self.query.weight_integer', 'ibert.encoder.layer.4.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.4.attention.self.query_activation.x_max', 'ibert.encoder.layer.4.attention.self.query_activation.x_min', 'ibert.encoder.layer.4.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.4.attention.self.softmax.act.x_max', 'ibert.encoder.layer.4.attention.self.softmax.act.x_min', 'ibert.encoder.layer.4.attention.self.value.bias_integer', 'ibert.encoder.layer.4.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.4.attention.self.value.weight_integer', 'ibert.encoder.layer.4.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.4.attention.self.value_activation.x_max', 'ibert.encoder.layer.4.attention.self.value_activation.x_min', 'ibert.encoder.layer.4.intermediate.dense.bias_integer', 'ibert.encoder.layer.4.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.4.intermediate.dense.weight_integer', 'ibert.encoder.layer.4.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.4.intermediate.output_activation.x_max', 'ibert.encoder.layer.4.intermediate.output_activation.x_min', 'ibert.encoder.layer.4.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.4.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.4.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.4.output.LayerNorm.shift', 'ibert.encoder.layer.4.output.dense.bias_integer', 'ibert.encoder.layer.4.output.dense.fc_scaling_factor', 'ibert.encoder.layer.4.output.dense.weight_integer', 'ibert.encoder.layer.4.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.4.output.ln_input_act.x_max', 'ibert.encoder.layer.4.output.ln_input_act.x_min', 'ibert.encoder.layer.4.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.4.output.output_activation.x_max', 'ibert.encoder.layer.4.output.output_activation.x_min', 'ibert.encoder.layer.4.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.4.pre_intermediate_act.x_max', 'ibert.encoder.layer.4.pre_intermediate_act.x_min', 'ibert.encoder.layer.4.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.4.pre_output_act.x_max', 'ibert.encoder.layer.4.pre_output_act.x_min', 'ibert.encoder.layer.5.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.5.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.5.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.5.attention.output.LayerNorm.shift', 'ibert.encoder.layer.5.attention.output.dense.bias_integer', 'ibert.encoder.layer.5.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.5.attention.output.dense.weight_integer', 'ibert.encoder.layer.5.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.5.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.5.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.5.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.5.attention.output.output_activation.x_max', 'ibert.encoder.layer.5.attention.output.output_activation.x_min', 'ibert.encoder.layer.5.attention.self.key.bias_integer', 'ibert.encoder.layer.5.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.5.attention.self.key.weight_integer', 'ibert.encoder.layer.5.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.5.attention.self.key_activation.x_max', 'ibert.encoder.layer.5.attention.self.key_activation.x_min', 'ibert.encoder.layer.5.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.5.attention.self.output_activation.x_max', 'ibert.encoder.layer.5.attention.self.output_activation.x_min', 'ibert.encoder.layer.5.attention.self.query.bias_integer', 'ibert.encoder.layer.5.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.5.attention.self.query.weight_integer', 'ibert.encoder.layer.5.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.5.attention.self.query_activation.x_max', 'ibert.encoder.layer.5.attention.self.query_activation.x_min', 'ibert.encoder.layer.5.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.5.attention.self.softmax.act.x_max', 'ibert.encoder.layer.5.attention.self.softmax.act.x_min', 'ibert.encoder.layer.5.attention.self.value.bias_integer', 'ibert.encoder.layer.5.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.5.attention.self.value.weight_integer', 'ibert.encoder.layer.5.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.5.attention.self.value_activation.x_max', 'ibert.encoder.layer.5.attention.self.value_activation.x_min', 'ibert.encoder.layer.5.intermediate.dense.bias_integer', 'ibert.encoder.layer.5.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.5.intermediate.dense.weight_integer', 'ibert.encoder.layer.5.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.5.intermediate.output_activation.x_max', 'ibert.encoder.layer.5.intermediate.output_activation.x_min', 'ibert.encoder.layer.5.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.5.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.5.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.5.output.LayerNorm.shift', 'ibert.encoder.layer.5.output.dense.bias_integer', 'ibert.encoder.layer.5.output.dense.fc_scaling_factor', 'ibert.encoder.layer.5.output.dense.weight_integer', 'ibert.encoder.layer.5.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.5.output.ln_input_act.x_max', 'ibert.encoder.layer.5.output.ln_input_act.x_min', 'ibert.encoder.layer.5.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.5.output.output_activation.x_max', 'ibert.encoder.layer.5.output.output_activation.x_min', 'ibert.encoder.layer.5.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.5.pre_intermediate_act.x_max', 'ibert.encoder.layer.5.pre_intermediate_act.x_min', 'ibert.encoder.layer.5.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.5.pre_output_act.x_max', 'ibert.encoder.layer.5.pre_output_act.x_min', 'ibert.encoder.layer.6.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.6.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.6.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.6.attention.output.LayerNorm.shift', 'ibert.encoder.layer.6.attention.output.dense.bias_integer', 'ibert.encoder.layer.6.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.6.attention.output.dense.weight_integer', 'ibert.encoder.layer.6.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.6.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.6.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.6.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.6.attention.output.output_activation.x_max', 'ibert.encoder.layer.6.attention.output.output_activation.x_min', 'ibert.encoder.layer.6.attention.self.key.bias_integer', 'ibert.encoder.layer.6.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.6.attention.self.key.weight_integer', 'ibert.encoder.layer.6.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.6.attention.self.key_activation.x_max', 'ibert.encoder.layer.6.attention.self.key_activation.x_min', 'ibert.encoder.layer.6.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.6.attention.self.output_activation.x_max', 'ibert.encoder.layer.6.attention.self.output_activation.x_min', 'ibert.encoder.layer.6.attention.self.query.bias_integer', 'ibert.encoder.layer.6.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.6.attention.self.query.weight_integer', 'ibert.encoder.layer.6.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.6.attention.self.query_activation.x_max', 'ibert.encoder.layer.6.attention.self.query_activation.x_min', 'ibert.encoder.layer.6.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.6.attention.self.softmax.act.x_max', 'ibert.encoder.layer.6.attention.self.softmax.act.x_min', 'ibert.encoder.layer.6.attention.self.value.bias_integer', 'ibert.encoder.layer.6.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.6.attention.self.value.weight_integer', 'ibert.encoder.layer.6.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.6.attention.self.value_activation.x_max', 'ibert.encoder.layer.6.attention.self.value_activation.x_min', 'ibert.encoder.layer.6.intermediate.dense.bias_integer', 'ibert.encoder.layer.6.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.6.intermediate.dense.weight_integer', 'ibert.encoder.layer.6.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.6.intermediate.output_activation.x_max', 'ibert.encoder.layer.6.intermediate.output_activation.x_min', 'ibert.encoder.layer.6.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.6.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.6.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.6.output.LayerNorm.shift', 'ibert.encoder.layer.6.output.dense.bias_integer', 'ibert.encoder.layer.6.output.dense.fc_scaling_factor', 'ibert.encoder.layer.6.output.dense.weight_integer', 'ibert.encoder.layer.6.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.6.output.ln_input_act.x_max', 'ibert.encoder.layer.6.output.ln_input_act.x_min', 'ibert.encoder.layer.6.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.6.output.output_activation.x_max', 'ibert.encoder.layer.6.output.output_activation.x_min', 'ibert.encoder.layer.6.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.6.pre_intermediate_act.x_max', 'ibert.encoder.layer.6.pre_intermediate_act.x_min', 'ibert.encoder.layer.6.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.6.pre_output_act.x_max', 'ibert.encoder.layer.6.pre_output_act.x_min', 'ibert.encoder.layer.7.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.7.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.7.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.7.attention.output.LayerNorm.shift', 'ibert.encoder.layer.7.attention.output.dense.bias_integer', 'ibert.encoder.layer.7.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.7.attention.output.dense.weight_integer', 'ibert.encoder.layer.7.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.7.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.7.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.7.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.7.attention.output.output_activation.x_max', 'ibert.encoder.layer.7.attention.output.output_activation.x_min', 'ibert.encoder.layer.7.attention.self.key.bias_integer', 'ibert.encoder.layer.7.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.7.attention.self.key.weight_integer', 'ibert.encoder.layer.7.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.7.attention.self.key_activation.x_max', 'ibert.encoder.layer.7.attention.self.key_activation.x_min', 'ibert.encoder.layer.7.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.7.attention.self.output_activation.x_max', 'ibert.encoder.layer.7.attention.self.output_activation.x_min', 'ibert.encoder.layer.7.attention.self.query.bias_integer', 'ibert.encoder.layer.7.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.7.attention.self.query.weight_integer', 'ibert.encoder.layer.7.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.7.attention.self.query_activation.x_max', 'ibert.encoder.layer.7.attention.self.query_activation.x_min', 'ibert.encoder.layer.7.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.7.attention.self.softmax.act.x_max', 'ibert.encoder.layer.7.attention.self.softmax.act.x_min', 'ibert.encoder.layer.7.attention.self.value.bias_integer', 'ibert.encoder.layer.7.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.7.attention.self.value.weight_integer', 'ibert.encoder.layer.7.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.7.attention.self.value_activation.x_max', 'ibert.encoder.layer.7.attention.self.value_activation.x_min', 'ibert.encoder.layer.7.intermediate.dense.bias_integer', 'ibert.encoder.layer.7.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.7.intermediate.dense.weight_integer', 'ibert.encoder.layer.7.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.7.intermediate.output_activation.x_max', 'ibert.encoder.layer.7.intermediate.output_activation.x_min', 'ibert.encoder.layer.7.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.7.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.7.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.7.output.LayerNorm.shift', 'ibert.encoder.layer.7.output.dense.bias_integer', 'ibert.encoder.layer.7.output.dense.fc_scaling_factor', 'ibert.encoder.layer.7.output.dense.weight_integer', 'ibert.encoder.layer.7.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.7.output.ln_input_act.x_max', 'ibert.encoder.layer.7.output.ln_input_act.x_min', 'ibert.encoder.layer.7.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.7.output.output_activation.x_max', 'ibert.encoder.layer.7.output.output_activation.x_min', 'ibert.encoder.layer.7.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.7.pre_intermediate_act.x_max', 'ibert.encoder.layer.7.pre_intermediate_act.x_min', 'ibert.encoder.layer.7.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.7.pre_output_act.x_max', 'ibert.encoder.layer.7.pre_output_act.x_min', 'ibert.encoder.layer.8.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.8.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.8.attention.output.LayerNorm.shift', 'ibert.encoder.layer.8.attention.output.dense.bias_integer', 'ibert.encoder.layer.8.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.8.attention.output.dense.weight_integer', 'ibert.encoder.layer.8.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.8.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.8.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.8.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.output.output_activation.x_max', 'ibert.encoder.layer.8.attention.output.output_activation.x_min', 'ibert.encoder.layer.8.attention.self.key.bias_integer', 'ibert.encoder.layer.8.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.8.attention.self.key.weight_integer', 'ibert.encoder.layer.8.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.self.key_activation.x_max', 'ibert.encoder.layer.8.attention.self.key_activation.x_min', 'ibert.encoder.layer.8.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.self.output_activation.x_max', 'ibert.encoder.layer.8.attention.self.output_activation.x_min', 'ibert.encoder.layer.8.attention.self.query.bias_integer', 'ibert.encoder.layer.8.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.8.attention.self.query.weight_integer', 'ibert.encoder.layer.8.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.self.query_activation.x_max', 'ibert.encoder.layer.8.attention.self.query_activation.x_min', 'ibert.encoder.layer.8.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.8.attention.self.softmax.act.x_max', 'ibert.encoder.layer.8.attention.self.softmax.act.x_min', 'ibert.encoder.layer.8.attention.self.value.bias_integer', 'ibert.encoder.layer.8.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.8.attention.self.value.weight_integer', 'ibert.encoder.layer.8.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.8.attention.self.value_activation.x_max', 'ibert.encoder.layer.8.attention.self.value_activation.x_min', 'ibert.encoder.layer.8.intermediate.dense.bias_integer', 'ibert.encoder.layer.8.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.8.intermediate.dense.weight_integer', 'ibert.encoder.layer.8.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.8.intermediate.output_activation.x_max', 'ibert.encoder.layer.8.intermediate.output_activation.x_min', 'ibert.encoder.layer.8.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.8.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.8.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.8.output.LayerNorm.shift', 'ibert.encoder.layer.8.output.dense.bias_integer', 'ibert.encoder.layer.8.output.dense.fc_scaling_factor', 'ibert.encoder.layer.8.output.dense.weight_integer', 'ibert.encoder.layer.8.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.8.output.ln_input_act.x_max', 'ibert.encoder.layer.8.output.ln_input_act.x_min', 'ibert.encoder.layer.8.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.8.output.output_activation.x_max', 'ibert.encoder.layer.8.output.output_activation.x_min', 'ibert.encoder.layer.8.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.8.pre_intermediate_act.x_max', 'ibert.encoder.layer.8.pre_intermediate_act.x_min', 'ibert.encoder.layer.8.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.8.pre_output_act.x_max', 'ibert.encoder.layer.8.pre_output_act.x_min', 'ibert.encoder.layer.9.attention.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.9.attention.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.9.attention.output.LayerNorm.shift', 'ibert.encoder.layer.9.attention.output.dense.bias_integer', 'ibert.encoder.layer.9.attention.output.dense.fc_scaling_factor', 'ibert.encoder.layer.9.attention.output.dense.weight_integer', 'ibert.encoder.layer.9.attention.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.9.attention.output.ln_input_act.x_max', 'ibert.encoder.layer.9.attention.output.ln_input_act.x_min', 'ibert.encoder.layer.9.attention.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.output.output_activation.x_max', 'ibert.encoder.layer.9.attention.output.output_activation.x_min', 'ibert.encoder.layer.9.attention.self.key.bias_integer', 'ibert.encoder.layer.9.attention.self.key.fc_scaling_factor', 'ibert.encoder.layer.9.attention.self.key.weight_integer', 'ibert.encoder.layer.9.attention.self.key_activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.self.key_activation.x_max', 'ibert.encoder.layer.9.attention.self.key_activation.x_min', 'ibert.encoder.layer.9.attention.self.output_activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.self.output_activation.x_max', 'ibert.encoder.layer.9.attention.self.output_activation.x_min', 'ibert.encoder.layer.9.attention.self.query.bias_integer', 'ibert.encoder.layer.9.attention.self.query.fc_scaling_factor', 'ibert.encoder.layer.9.attention.self.query.weight_integer', 'ibert.encoder.layer.9.attention.self.query_activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.self.query_activation.x_max', 'ibert.encoder.layer.9.attention.self.query_activation.x_min', 'ibert.encoder.layer.9.attention.self.softmax.act.act_scaling_factor', 'ibert.encoder.layer.9.attention.self.softmax.act.x_max', 'ibert.encoder.layer.9.attention.self.softmax.act.x_min', 'ibert.encoder.layer.9.attention.self.value.bias_integer', 'ibert.encoder.layer.9.attention.self.value.fc_scaling_factor', 'ibert.encoder.layer.9.attention.self.value.weight_integer', 'ibert.encoder.layer.9.attention.self.value_activation.act_scaling_factor', 'ibert.encoder.layer.9.attention.self.value_activation.x_max', 'ibert.encoder.layer.9.attention.self.value_activation.x_min', 'ibert.encoder.layer.9.intermediate.dense.bias_integer', 'ibert.encoder.layer.9.intermediate.dense.fc_scaling_factor', 'ibert.encoder.layer.9.intermediate.dense.weight_integer', 'ibert.encoder.layer.9.intermediate.output_activation.act_scaling_factor', 'ibert.encoder.layer.9.intermediate.output_activation.x_max', 'ibert.encoder.layer.9.intermediate.output_activation.x_min', 'ibert.encoder.layer.9.output.LayerNorm.activation.act_scaling_factor', 'ibert.encoder.layer.9.output.LayerNorm.activation.x_max', 'ibert.encoder.layer.9.output.LayerNorm.activation.x_min', 'ibert.encoder.layer.9.output.LayerNorm.shift', 'ibert.encoder.layer.9.output.dense.bias_integer', 'ibert.encoder.layer.9.output.dense.fc_scaling_factor', 'ibert.encoder.layer.9.output.dense.weight_integer', 'ibert.encoder.layer.9.output.ln_input_act.act_scaling_factor', 'ibert.encoder.layer.9.output.ln_input_act.x_max', 'ibert.encoder.layer.9.output.ln_input_act.x_min', 'ibert.encoder.layer.9.output.output_activation.act_scaling_factor', 'ibert.encoder.layer.9.output.output_activation.x_max', 'ibert.encoder.layer.9.output.output_activation.x_min', 'ibert.encoder.layer.9.pre_intermediate_act.act_scaling_factor', 'ibert.encoder.layer.9.pre_intermediate_act.x_max', 'ibert.encoder.layer.9.pre_intermediate_act.x_min', 'ibert.encoder.layer.9.pre_output_act.act_scaling_factor', 'ibert.encoder.layer.9.pre_output_act.x_max', 'ibert.encoder.layer.9.pre_output_act.x_min']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBertForSequenceClassification(\n",
      "  (ibert): IBertModel(\n",
      "    (embeddings): IBertEmbeddings(\n",
      "      (word_embeddings): QuantEmbedding()\n",
      "      (token_type_embeddings): QuantEmbedding()\n",
      "      (position_embeddings): QuantEmbedding()\n",
      "      (embeddings_act1): QuantAct(activation_bit=16, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "      (embeddings_act2): QuantAct(activation_bit=16, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "      (LayerNorm): IntLayerNorm(\n",
      "        (activation): QuantAct(activation_bit=32, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "      )\n",
      "      (output_activation): QuantAct(activation_bit=8, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): IBertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x IBertLayer(\n",
      "          (attention): IBertAttention(\n",
      "            (self): IBertSelfAttention(\n",
      "              (query): (QuantLinear() weight_bit=8, quant_mode=False)\n",
      "              (key): (QuantLinear() weight_bit=8, quant_mode=False)\n",
      "              (value): (QuantLinear() weight_bit=8, quant_mode=False)\n",
      "              (query_activation): QuantAct(activation_bit=8, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "              (key_activation): QuantAct(activation_bit=8, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "              (value_activation): QuantAct(activation_bit=8, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "              (output_activation): QuantAct(activation_bit=8, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (softmax): IntSoftmax(\n",
      "                (act): QuantAct(activation_bit=16, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "              )\n",
      "            )\n",
      "            (output): IBertSelfOutput(\n",
      "              (dense): (QuantLinear() weight_bit=8, quant_mode=False)\n",
      "              (ln_input_act): QuantAct(activation_bit=22, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "              (LayerNorm): IntLayerNorm(\n",
      "                (activation): QuantAct(activation_bit=32, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "              )\n",
      "              (output_activation): QuantAct(activation_bit=8, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): IBertIntermediate(\n",
      "            (dense): (QuantLinear() weight_bit=8, quant_mode=False)\n",
      "            (intermediate_act_fn): IntGELU(\n",
      "              (activation_fn): GELU(approximate='none')\n",
      "            )\n",
      "            (output_activation): QuantAct(activation_bit=8, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "          )\n",
      "          (output): IBertOutput(\n",
      "            (dense): (QuantLinear() weight_bit=8, quant_mode=False)\n",
      "            (ln_input_act): QuantAct(activation_bit=22, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "            (LayerNorm): IntLayerNorm(\n",
      "              (activation): QuantAct(activation_bit=32, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "            )\n",
      "            (output_activation): QuantAct(activation_bit=8, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (pre_intermediate_act): QuantAct(activation_bit=8, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "          (pre_output_act): QuantAct(activation_bit=8, quant_mode: False, Act_min: -0.00, Act_max: 0.00)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): IBertClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AutoModelForSequenceClassification, DistilBertConfig, DataCollatorWithPadding\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels = 2,\n",
    "    #id2label = id2label,\n",
    "    #label2id = label2id,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'weight': tensor([[ 0.0729, -0.0029, -0.0902,  ...,  0.1033,  0.0900, -0.1030],\n",
      "        [-0.0516,  0.2061,  0.0739,  ...,  0.0657,  0.0634,  0.1282],\n",
      "        [ 0.0878,  0.0698, -0.0515,  ..., -0.0426, -0.0081,  0.1100],\n",
      "        ...,\n",
      "        [-0.1871,  0.0172, -0.0315,  ..., -0.0503,  0.1024, -0.1165],\n",
      "        [-0.2532,  0.0439,  0.0638,  ...,  0.0701, -0.1045,  0.0118],\n",
      "        [-0.0516, -0.0859,  0.1027,  ..., -0.1895,  0.0033, -0.0541]]), 'bias': tensor([ 2.3572e-01,  4.6570e-02, -3.7012e-01,  4.8804e-01,  7.1484e-01,\n",
      "        -2.8931e-01, -9.8755e-02,  2.5928e-01, -4.6722e-02,  3.9771e-01,\n",
      "         5.5127e-01,  5.8008e-01,  1.5674e-01, -1.7273e-01,  5.7129e-01,\n",
      "        -2.9053e-01, -2.1350e-01,  9.8450e-02,  2.4033e-02, -9.2983e-04,\n",
      "        -5.0195e-01,  1.0547e-01, -4.2847e-02, -1.1133e-01, -6.2988e-01,\n",
      "         1.7786e-01,  1.2720e-01, -3.9453e-01, -5.3345e-02,  3.4106e-01,\n",
      "         6.4270e-02, -3.5278e-01, -2.7515e-01, -3.8574e-01,  9.6008e-02,\n",
      "        -1.8677e-01,  6.0742e-01,  8.0322e-02,  1.0938e-01,  1.9165e-01,\n",
      "        -6.8909e-02,  1.3098e-01,  9.3994e-02, -4.5807e-02,  3.2861e-01,\n",
      "        -7.5562e-02,  2.7051e-01,  2.1216e-01,  1.4062e-01, -4.0356e-01,\n",
      "         2.7246e-01,  1.1865e-01, -4.1919e-01,  8.1787e-02, -5.4980e-01,\n",
      "        -2.9272e-01,  4.8065e-02,  1.3293e-01, -4.1162e-01,  1.7456e-02,\n",
      "         5.2637e-01, -1.1560e-01, -4.8340e-01,  1.7859e-01,  2.0142e-01,\n",
      "        -4.0845e-01, -5.5161e-03, -3.7524e-01,  2.6270e-01,  9.4482e-02,\n",
      "         2.7637e-01, -8.6792e-02,  2.8638e-01,  8.5144e-02,  5.1318e-01,\n",
      "        -6.4209e-01,  7.4951e-02, -6.7676e-01,  1.1273e-01,  5.0488e-01,\n",
      "        -5.3070e-02, -3.1665e-01,  8.5144e-02, -3.4717e-01, -1.4172e-01,\n",
      "        -2.3117e-02,  1.8628e-01, -6.7787e-03, -2.3730e-01,  1.9214e-01,\n",
      "         1.7493e-01,  8.7158e-02,  1.0382e-01,  8.5632e-02,  8.5068e-03,\n",
      "        -4.0649e-01, -1.8127e-01,  1.1285e-01, -5.2887e-02, -4.2554e-01,\n",
      "        -1.8213e-01,  1.0706e-01,  2.8061e-02,  3.9917e-01, -3.4961e-01,\n",
      "         1.3107e-02, -1.7395e-01, -6.5186e-02, -4.9243e-01, -6.4659e-03,\n",
      "         5.9326e-01, -5.0079e-02, -5.2295e-01, -4.1821e-01, -4.5410e-02,\n",
      "         8.4000e-03, -5.6982e-01,  3.0347e-01,  1.2537e-01, -8.4717e-02,\n",
      "         1.1182e-01, -1.4966e-01, -8.7585e-02, -1.4026e-01, -3.5986e-01,\n",
      "        -1.1621e-01,  3.6102e-02, -1.4771e-01,  2.4524e-01, -3.0444e-01,\n",
      "        -5.1855e-01, -1.2781e-01, -2.6440e-01, -2.7130e-02, -1.2091e-01,\n",
      "        -2.3926e-01,  2.2253e-01, -1.6943e-01, -2.1240e-01,  2.3633e-01,\n",
      "         3.7427e-01,  5.3940e-03, -1.6809e-01, -9.0515e-02,  3.3936e-01,\n",
      "        -3.8110e-01, -4.1699e-01, -6.2439e-02,  2.0569e-01,  1.9608e-02,\n",
      "         1.1833e-02, -6.3660e-02,  1.5393e-01,  9.8816e-02, -3.9185e-01,\n",
      "         1.7432e-01,  1.3257e-01, -2.2839e-01, -6.6162e-02,  5.5176e-01,\n",
      "         2.9785e-01,  1.1682e-01,  5.3040e-02,  8.8257e-02, -8.0444e-02,\n",
      "         1.5540e-01, -3.3350e-01,  7.3547e-03, -3.3887e-01,  1.6931e-01,\n",
      "         9.8511e-02,  2.6855e-01, -7.3975e-02, -1.1652e-01, -2.9907e-03,\n",
      "         3.1055e-01, -3.6621e-01,  9.1003e-02, -2.8735e-01, -1.4610e-02,\n",
      "         3.7134e-01, -6.7566e-02, -2.2327e-01, -2.3279e-01, -1.5247e-01,\n",
      "         2.0410e-01, -2.8369e-01, -2.9346e-01, -6.9275e-02, -1.6821e-01,\n",
      "        -1.0815e-01,  2.6855e-01, -5.3040e-02,  1.4246e-01, -1.6968e-01,\n",
      "         1.3196e-01,  1.1139e-01,  1.2866e-01,  1.0059e-01,  1.1487e-01,\n",
      "        -1.8335e-01, -1.5613e-01, -2.3730e-01,  4.8248e-02,  1.6434e-02,\n",
      "         1.6321e-01,  7.6218e-03,  3.2178e-01, -1.9958e-02,  9.8083e-02,\n",
      "        -2.6722e-03, -6.3538e-02, -7.8247e-02,  1.9519e-01, -1.3855e-01,\n",
      "         3.2690e-01,  9.4360e-02,  4.1577e-01,  8.6288e-03,  5.3711e-02,\n",
      "        -4.8096e-02, -4.4775e-01,  1.2290e-04,  3.1223e-03,  5.1514e-02,\n",
      "        -1.0608e-01,  9.8816e-02,  1.2585e-01, -2.8052e-01,  2.9617e-02,\n",
      "        -5.5023e-02,  1.1450e-01, -3.7689e-02, -4.2261e-01,  1.2079e-01,\n",
      "        -1.9116e-01, -2.6807e-01,  4.1718e-02, -1.2042e-01,  8.4045e-02,\n",
      "        -1.8494e-01, -1.9897e-01, -8.5083e-02, -3.1079e-01,  7.1960e-02,\n",
      "        -3.1567e-01, -3.4326e-01,  1.8604e-01, -5.3284e-02, -2.3022e-01,\n",
      "         1.6460e-03,  1.4429e-01, -1.3611e-01, -1.2720e-01, -2.8613e-01,\n",
      "        -1.0046e-01, -1.4429e-01,  2.2717e-01,  2.9395e-01,  1.7181e-02,\n",
      "         1.9385e-01,  3.7262e-02, -1.4557e-02,  2.7905e-01, -7.4402e-02,\n",
      "         4.2822e-01,  1.4551e-01, -7.1350e-02,  8.9661e-02, -2.0020e-02,\n",
      "         1.9434e-01,  5.5267e-02,  3.6157e-01, -2.5391e-01,  1.2140e-01,\n",
      "        -1.3184e-01, -1.3049e-01,  1.5161e-01,  4.7058e-02, -3.1128e-01,\n",
      "         4.5166e-01,  2.8442e-01,  2.0117e-01, -1.1346e-01,  1.8311e-01,\n",
      "        -1.1786e-01, -4.8438e-01,  2.9468e-01, -4.3182e-02, -3.6011e-02,\n",
      "        -3.1177e-01, -5.0146e-01, -8.9844e-02, -2.1033e-01,  2.6685e-01,\n",
      "        -1.8567e-01, -2.3694e-01,  5.0316e-03,  3.8071e-03,  9.9060e-02,\n",
      "         2.4304e-01,  4.7900e-01, -1.8518e-01, -3.3008e-01, -7.4707e-02,\n",
      "        -8.2458e-02, -3.3301e-01, -1.0138e-01,  7.9041e-02,  4.2725e-01,\n",
      "        -3.2910e-01, -4.6753e-01, -1.0358e-01,  4.8242e-01, -3.5156e-02,\n",
      "         1.4849e-03,  1.5515e-01, -7.8979e-02,  2.0093e-01, -3.0225e-01,\n",
      "        -2.5513e-01, -4.3164e-01, -3.6304e-01, -3.7646e-01,  2.3291e-01,\n",
      "        -3.1348e-01,  7.2266e-02,  1.5674e-01,  2.7847e-02, -2.5977e-01,\n",
      "        -4.2334e-01, -5.2197e-01, -3.3130e-01, -2.2302e-01, -1.5649e-01,\n",
      "         5.6787e-01,  2.3779e-01, -1.6953e-02, -2.1408e-02,  4.9829e-01,\n",
      "        -3.2959e-02, -6.8298e-02,  5.0195e-01,  8.3435e-02, -5.0439e-01,\n",
      "         4.4287e-01, -7.8430e-02,  5.3619e-02, -3.5669e-01,  3.6865e-01,\n",
      "         4.2603e-01, -1.7334e-01,  3.4570e-01, -4.7412e-01, -1.5161e-01,\n",
      "         4.9866e-02,  1.5190e-02,  5.0586e-01,  4.9951e-01, -2.1729e-01,\n",
      "         5.0635e-01, -4.9268e-01, -2.6016e-02,  1.4526e-01, -5.0049e-01,\n",
      "        -6.7253e-03,  9.6191e-02,  4.8706e-01,  5.3520e-03, -5.2734e-01,\n",
      "         8.1238e-02, -8.8272e-03,  1.8604e-01,  4.9097e-01,  5.2344e-01,\n",
      "        -2.0288e-01, -5.5176e-01, -4.5581e-01, -3.4082e-01, -4.9585e-01,\n",
      "        -4.4409e-01,  2.6123e-01, -2.4829e-01,  2.2571e-01,  1.2427e-01,\n",
      "        -9.5886e-02,  3.3783e-02,  4.2847e-01,  3.3154e-01, -4.4922e-01,\n",
      "         1.9946e-01, -1.5137e-01, -2.6538e-01,  5.7526e-02,  1.1713e-01,\n",
      "        -8.9172e-02,  5.0879e-01,  1.3525e-01,  7.5378e-02, -2.5742e-02,\n",
      "        -4.1260e-01, -5.8057e-01,  5.8533e-02,  1.8921e-01, -7.8430e-02,\n",
      "        -6.3965e-01, -7.5195e-02, -2.4634e-01, -3.7445e-02,  5.4297e-01,\n",
      "        -2.5024e-01,  5.2643e-02, -4.2358e-02, -4.5752e-01, -4.6484e-01,\n",
      "        -3.4619e-01, -1.0992e-01,  4.4751e-01, -1.7639e-01, -2.2473e-01,\n",
      "         2.2607e-01,  4.6448e-02, -5.0293e-01, -6.7017e-02, -3.1677e-02,\n",
      "         1.5393e-01, -9.3079e-02, -5.7031e-01, -3.7280e-01,  9.9365e-02,\n",
      "        -5.0244e-01,  1.5540e-01, -6.3171e-02, -1.8359e-01, -8.1604e-02,\n",
      "         6.9031e-02, -3.4375e-01,  5.5225e-01, -2.5269e-01, -1.5820e-01,\n",
      "        -2.7222e-01,  1.2561e-01, -6.5552e-02, -7.8201e-03, -1.7957e-01,\n",
      "         6.0742e-01,  3.1030e-01,  6.3858e-03,  4.4342e-02, -1.4502e-01,\n",
      "        -2.2705e-01, -2.5366e-01,  1.8425e-03, -3.5620e-01,  2.2070e-01,\n",
      "         5.8929e-02,  1.9019e-01,  1.9348e-02, -1.9043e-01,  1.2805e-01,\n",
      "        -3.2544e-01, -2.6855e-01,  5.1123e-01, -1.8738e-01,  1.6296e-01,\n",
      "        -1.7590e-01, -2.7319e-01, -3.5400e-01, -1.4575e-01,  1.3147e-01,\n",
      "        -3.0615e-01, -3.0054e-01,  1.8051e-02,  3.9331e-01,  3.7695e-01,\n",
      "         1.5967e-01,  2.0715e-01,  5.1904e-01,  7.6721e-02,  4.2053e-02,\n",
      "        -1.1157e-01,  6.3400e-03, -1.8323e-01,  1.8970e-01,  3.4717e-01,\n",
      "        -1.9592e-01, -1.0657e-01, -2.2791e-01, -1.6382e-01, -1.1658e-01,\n",
      "        -1.7957e-01, -2.9150e-01,  8.8684e-02,  5.0342e-01, -3.5034e-01,\n",
      "         2.4036e-01,  8.2520e-02, -3.6426e-01, -3.2837e-01,  4.3915e-02,\n",
      "        -3.7524e-01, -4.3457e-01,  3.4473e-01,  6.0028e-02, -2.6001e-01,\n",
      "         2.2095e-02,  2.4182e-01,  8.0139e-02, -1.9824e-01, -2.8662e-01,\n",
      "        -7.2632e-02,  1.9543e-01, -1.2415e-01, -4.5410e-01, -8.1787e-02,\n",
      "        -4.2236e-02, -2.1924e-01,  1.1560e-01, -9.4971e-02,  5.3894e-02,\n",
      "         1.7529e-01, -4.7951e-03,  8.9722e-02,  1.6541e-01,  3.1342e-02,\n",
      "         2.2363e-01, -1.5507e-03,  2.3499e-02,  2.2131e-01,  7.5111e-03,\n",
      "         1.3281e-01, -2.8149e-01,  2.3755e-01,  8.0383e-02, -2.4878e-01,\n",
      "        -1.9543e-01,  2.8320e-01,  1.7468e-01, -2.3645e-01,  4.5190e-01,\n",
      "         1.3025e-01,  5.0391e-01,  4.7754e-01,  1.9141e-01,  1.2291e-02,\n",
      "         5.2124e-02,  2.8271e-01, -1.8994e-01, -8.1604e-02, -2.0996e-01,\n",
      "        -5.0751e-02, -2.0422e-01, -4.0283e-01, -1.0468e-01,  1.4038e-01,\n",
      "        -3.6392e-03, -3.6957e-02, -2.4475e-01, -1.8323e-01, -1.3110e-01,\n",
      "         2.3962e-01,  1.2500e-01,  2.1210e-02, -3.1885e-01, -3.6084e-01,\n",
      "         2.3792e-01, -7.7271e-02, -4.4580e-01,  6.8481e-02,  2.0862e-01,\n",
      "        -8.6021e-04,  5.0586e-01,  1.7688e-01, -3.1519e-01,  2.9712e-01,\n",
      "         5.3955e-02,  2.6627e-03, -1.2744e-01,  5.1221e-01,  5.5957e-01,\n",
      "        -1.8994e-01, -3.3569e-01,  3.9856e-02,  5.0684e-01, -5.8252e-01,\n",
      "         8.0872e-02,  4.5288e-01, -3.2520e-01, -5.8398e-01,  4.0894e-01,\n",
      "         4.5605e-01, -6.0693e-01,  2.1509e-01, -1.2085e-01, -1.8921e-01,\n",
      "        -4.6509e-01, -6.1719e-01,  4.0454e-01,  1.3824e-02, -6.9043e-01,\n",
      "         3.5864e-01, -1.7712e-01, -4.4629e-01,  5.9961e-01, -1.4258e-01,\n",
      "        -7.4036e-02, -4.9341e-01,  5.3027e-01,  5.5566e-01, -4.8950e-01,\n",
      "        -5.5566e-01,  6.4990e-01, -5.2783e-01,  4.6729e-01, -1.2317e-01,\n",
      "        -4.4116e-01,  4.9487e-01, -5.2441e-01, -2.0496e-01,  3.0005e-01,\n",
      "        -6.0693e-01, -2.1106e-01,  3.8672e-01,  1.2756e-01,  4.4751e-01,\n",
      "        -5.5127e-01, -1.7105e-02,  2.7759e-01,  3.1055e-01, -1.2018e-01,\n",
      "        -3.1836e-01,  2.2751e-02, -4.7827e-01,  5.2295e-01, -2.4109e-01,\n",
      "         4.2822e-01,  3.2593e-01,  6.1340e-02, -4.6826e-01, -2.4182e-01,\n",
      "         1.2354e-01, -3.2178e-01, -3.3691e-01, -3.4180e-01,  3.4058e-01,\n",
      "         1.9989e-02, -1.0162e-01,  4.2700e-01, -4.0283e-01, -5.5145e-02,\n",
      "        -1.7334e-02, -1.9812e-01,  3.2764e-01, -3.3618e-01,  1.2109e-01,\n",
      "         1.7078e-01, -3.4375e-01, -4.2627e-01, -2.1118e-01, -3.5962e-01,\n",
      "        -3.5449e-01, -5.5237e-02,  3.5522e-01, -1.4490e-01,  3.1519e-01,\n",
      "         5.4102e-01, -3.2520e-01, -2.8290e-02, -7.3051e-04,  3.8672e-01,\n",
      "        -1.8201e-01,  7.3181e-02, -2.1509e-01,  5.2783e-01, -8.0750e-02,\n",
      "         2.0203e-01, -1.4062e-01, -1.6992e-01, -8.9340e-03,  2.9028e-01,\n",
      "         4.4342e-02,  2.0203e-01, -9.8511e-02, -6.0181e-02, -2.0630e-01,\n",
      "        -3.7793e-01, -3.6426e-01,  1.7639e-01,  4.2236e-01,  3.5553e-02,\n",
      "        -1.5771e-01, -3.8281e-01, -2.3376e-02, -6.7177e-03, -2.6270e-01,\n",
      "         2.3560e-01, -2.6440e-01, -7.8735e-02,  3.2861e-01,  2.4255e-01,\n",
      "        -6.0974e-02, -4.1138e-01, -2.6831e-01, -3.2422e-01, -8.6133e-01,\n",
      "         9.1260e-01, -5.0244e-01,  1.0413e-01, -5.4108e-02,  5.7568e-01,\n",
      "         7.7588e-01, -8.2520e-01, -9.9902e-01,  8.8770e-01, -1.0020e+00,\n",
      "         8.3496e-01, -1.7126e-01, -4.9927e-01, -2.8198e-01, -1.0114e-01,\n",
      "        -8.4863e-01, -8.9502e-01,  8.1201e-01,  8.8135e-01,  8.1689e-01,\n",
      "        -2.6611e-01, -6.9482e-01, -5.1611e-01,  6.8311e-01,  7.5342e-01,\n",
      "        -8.4717e-01, -7.8491e-02, -8.4619e-01,  9.5947e-01,  8.8477e-01,\n",
      "        -7.6025e-01,  7.9248e-01,  5.2979e-01, -7.7930e-01,  7.4072e-01,\n",
      "         4.3970e-01,  6.8506e-01, -5.6836e-01, -7.7441e-01, -7.1289e-01,\n",
      "         9.2432e-01, -9.9707e-01,  8.4082e-01,  7.3193e-01,  4.0308e-01,\n",
      "        -9.9902e-01,  6.3135e-01,  8.0225e-01, -8.6768e-01,  6.9727e-01,\n",
      "         4.0454e-01,  5.3711e-01, -6.8994e-01,  5.1514e-01,  7.3682e-01,\n",
      "         6.7920e-01, -6.6699e-01,  1.8420e-01, -8.1543e-01,  5.0439e-01,\n",
      "        -7.0850e-01,  1.0000e+00,  7.6318e-01]), 'weight_integer': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'fc_scaling_factor': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'bias_integer': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])})\n"
     ]
    }
   ],
   "source": [
    "print(model.ibert.encoder.layer[0].attention.self.query.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Preprocess the logits to ensure they are in the correct format for metric computation.\n",
    "    This function will be called during the evaluation process.\n",
    "    \"\"\"\n",
    "    if isinstance(logits, tuple):  \n",
    "        logits = logits[0]  # get logit tensors\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    return pred_ids, labels\n",
    "    \n",
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    return accuracy.compute(predictions=predictions[0], references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21768/1914963700.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.00005\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './imdb_tune_distilbert',\n",
    "    num_train_epochs = EPOCHS,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    logging_dir = './logs',\n",
    "    load_best_model_at_end= True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps = 500,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=['tensorboard'],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=small_train_dataset,         \n",
    "    eval_dataset=small_eval_dataset.shuffle(seed=72).select(range(600)),\n",
    "    compute_metrics = compute_metrics,\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 33:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.182836</td>\n",
       "      <td>0.941500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.39196124267578125, metrics={'train_runtime': 2017.9639, 'train_samples_per_second': 0.991, 'train_steps_per_second': 0.062, 'total_flos': 264934797312000.0, 'train_loss': 0.39196124267578125, 'epoch': 1.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/se/coding/school/exjobb/mycode/.venv/lib/python3.12/site-packages/transformers/training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 02:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after quantization: 0.48833333333333334\n"
     ]
    }
   ],
   "source": [
    "untrained = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels = num_labels,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    ")\n",
    "\n",
    "training_args_untrained = TrainingArguments(\n",
    "    output_dir=\"./results\",  \n",
    "    per_device_eval_batch_size=2, \n",
    "    use_cpu=True,  \n",
    "    no_cuda = True,\n",
    "    logging_dir=\"./logs\",  \n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer_untrained = Trainer(\n",
    "    model=untrained,\n",
    "    args=training_args_untrained,\n",
    "    eval_dataset=small_eval_dataset.select(range(600)),\n",
    "    data_collator = data_collator,\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"accuracy no training: \", trainer_untrained.evaluate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 04:57]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy trained after quant of weights to int8  {'eval_loss': 0.1721130609512329, 'eval_model_preparation_time': 0.0022, 'eval_accuracy': 0.94, 'eval_runtime': 127.8146, 'eval_samples_per_second': 4.694, 'eval_steps_per_second': 0.297}\n"
     ]
    }
   ],
   "source": [
    "#quantize trained imdb model\n",
    "from optimum.quanto import freeze, quantize, qint8\n",
    "\n",
    "\n",
    "\n",
    "print(\"accuracy trained after quant of weights to int8 \", trainer.evaluate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy trained after quant of weights to int8  {'eval_loss': 0.17312778532505035, 'eval_model_preparation_time': 0.0022, 'eval_accuracy': 0.94, 'eval_runtime': 155.7452, 'eval_samples_per_second': 3.852, 'eval_steps_per_second': 0.244}\n"
     ]
    }
   ],
   "source": [
    "quantize(model, weights=qint8, activations=None)\n",
    "freeze(model)\n",
    "print(\"accuracy trained after quant of weights to int8 \", trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): QLinear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): QLinear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): QLinear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): QLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): QLinear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): QLinear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): QLinear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): QLinear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantize(model)\n",
    "trainer.train()\n",
    "freeze(model)\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
