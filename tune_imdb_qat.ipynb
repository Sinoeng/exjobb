{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/coding/exjobb/.venv/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import DistilBertModel, DistilBertForMaskedLM, DistilBertTokenizer\n",
    "#from optimum.quanto import freeze, quantize, qint8, WeightQBytesTensor\n",
    "import datasets\n",
    "from transformers import TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"imdb\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    tokens = tokenizer(data[\"text\"], truncation=True, padding = 'max_length',  max_length=512)\n",
    "    tokens[\"label\"] = data[\"label\"]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dataset.map(preprocess, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokens['train'].features['label'].names\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = {}, {}\n",
    "\n",
    "for idx, lbl in enumerate(labels):\n",
    "    label2id[lbl] = idx\n",
    "    id2label[idx] = lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AutoModelForSequenceClassification, DistilBertConfig, DataCollatorWithPadding\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels = num_labels,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchao.quantization import (\n",
    "    quantize_,\n",
    "    Int8DynamicActivationInt4WeightConfig,\n",
    ")\n",
    "from torchao.quantization.qat import (\n",
    "    FakeQuantizeConfig,\n",
    "    FromIntXQuantizationAwareTrainingConfig,\n",
    "    IntXQuantizationAwareTrainingConfig,\n",
    ")\n",
    "\n",
    "activation_config = FakeQuantizeConfig(torch.int8, \"per_token\", is_symmetric=False)\n",
    "weight_config = FakeQuantizeConfig(torch.int4, group_size=32)\n",
    "\n",
    "quantize_(\n",
    "    model,\n",
    "    IntXQuantizationAwareTrainingConfig(activation_config, weight_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "100%|██████████| 13/13 [01:34<00:00,  6.09s/it]"
     ]
    }
   ],
   "source": [
    "device  = 'cpu'\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.00005\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './imdb_tune_distilbert_qat',\n",
    "    num_train_epochs = EPOCHS,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    logging_dir = './logs',\n",
    "    load_best_model_at_end= True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps = 500,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=['tensorboard'],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=tokens[\"train\"].shuffle(seed=11),         \n",
    "    eval_dataset=tokens[\"test\"].shuffle(seed=72).select(range(5000)),\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): FakeQuantizedLinear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (activation_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int8, granularity=PerToken(), mapping_type=<MappingType.ASYMMETRIC: 3>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "              (weight_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int4, granularity=PerGroup(group_size=32), mapping_type=<MappingType.SYMMETRIC: 1>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "            )\n",
      "            (k_lin): FakeQuantizedLinear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (activation_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int8, granularity=PerToken(), mapping_type=<MappingType.ASYMMETRIC: 3>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "              (weight_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int4, granularity=PerGroup(group_size=32), mapping_type=<MappingType.SYMMETRIC: 1>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "            )\n",
      "            (v_lin): FakeQuantizedLinear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (activation_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int8, granularity=PerToken(), mapping_type=<MappingType.ASYMMETRIC: 3>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "              (weight_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int4, granularity=PerGroup(group_size=32), mapping_type=<MappingType.SYMMETRIC: 1>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "            )\n",
      "            (out_lin): FakeQuantizedLinear(\n",
      "              in_features=768, out_features=768, bias=True\n",
      "              (activation_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int8, granularity=PerToken(), mapping_type=<MappingType.ASYMMETRIC: 3>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "              (weight_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int4, granularity=PerGroup(group_size=32), mapping_type=<MappingType.SYMMETRIC: 1>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "            )\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): FakeQuantizedLinear(\n",
      "              in_features=768, out_features=3072, bias=True\n",
      "              (activation_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int8, granularity=PerToken(), mapping_type=<MappingType.ASYMMETRIC: 3>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "              (weight_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int4, granularity=PerGroup(group_size=32), mapping_type=<MappingType.SYMMETRIC: 1>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "            )\n",
      "            (lin2): FakeQuantizedLinear(\n",
      "              in_features=3072, out_features=768, bias=True\n",
      "              (activation_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int8, granularity=PerToken(), mapping_type=<MappingType.ASYMMETRIC: 3>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "              (weight_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int4, granularity=PerGroup(group_size=32), mapping_type=<MappingType.SYMMETRIC: 1>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "            )\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): FakeQuantizedLinear(\n",
      "    in_features=768, out_features=768, bias=True\n",
      "    (activation_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int8, granularity=PerToken(), mapping_type=<MappingType.ASYMMETRIC: 3>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "    (weight_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int4, granularity=PerGroup(group_size=32), mapping_type=<MappingType.SYMMETRIC: 1>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "  )\n",
      "  (classifier): FakeQuantizedLinear(\n",
      "    in_features=768, out_features=2, bias=True\n",
      "    (activation_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int8, granularity=PerToken(), mapping_type=<MappingType.ASYMMETRIC: 3>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "    (weight_fake_quantizer): FakeQuantizer(FakeQuantizeConfig(dtype=torch.int4, granularity=PerGroup(group_size=32), mapping_type=<MappingType.SYMMETRIC: 1>, scale_precision=torch.float32, zero_point_precision=torch.int32, zero_point_domain=<ZeroPointDomain.INT: 1>, is_dynamic=True, range_learning=False))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#model = qat_quantizer.convert(model)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_asymm_per_token_quant at 0x7fc6bebe3380>, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 32), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=-8, quant_max=7)))\n",
      "            (k_lin): Linear(in_features=768, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_asymm_per_token_quant at 0x7fc6bebe3380>, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 32), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=-8, quant_max=7)))\n",
      "            (v_lin): Linear(in_features=768, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_asymm_per_token_quant at 0x7fc6bebe3380>, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 32), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=-8, quant_max=7)))\n",
      "            (out_lin): Linear(in_features=768, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_asymm_per_token_quant at 0x7fc6bebe3380>, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 32), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=-8, quant_max=7)))\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, weight=LinearActivationQuantizedTensor(activation=<function _int8_asymm_per_token_quant at 0x7fc6bebe3380>, weight=AffineQuantizedTensor(shape=torch.Size([3072, 768]), block_size=(1, 32), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=-8, quant_max=7)))\n",
      "            (lin2): Linear(in_features=3072, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_asymm_per_token_quant at 0x7fc6bebe3380>, weight=AffineQuantizedTensor(shape=torch.Size([768, 3072]), block_size=(1, 32), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=-8, quant_max=7)))\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, weight=LinearActivationQuantizedTensor(activation=<function _int8_asymm_per_token_quant at 0x7fc6bebe3380>, weight=AffineQuantizedTensor(shape=torch.Size([768, 768]), block_size=(1, 32), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=-8, quant_max=7)))\n",
      "  (classifier): Linear(in_features=768, out_features=2, weight=LinearActivationQuantizedTensor(activation=<function _int8_asymm_per_token_quant at 0x7fc6bebe3380>, weight=AffineQuantizedTensor(shape=torch.Size([2, 768]), block_size=(1, 32), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=-8, quant_max=7)))\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "quantize_(model, FromIntXQuantizationAwareTrainingConfig())\n",
    "quantize_(model, Int8DynamicActivationInt4WeightConfig(group_size=32))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'weight': LinearActivationQuantizedTensor(AffineQuantizedTensor(tensor_impl=PlainAQTTensorImpl(data=tensor([[ 0,  2, -2,  ...,  2,  6,  5],\n",
      "        [ 1,  2, -4,  ..., -1,  7,  1],\n",
      "        [ 0,  3,  2,  ..., -1,  1, -5],\n",
      "        ...,\n",
      "        [ 1,  0,  4,  ...,  3,  2, -1],\n",
      "        [ 0,  6,  6,  ..., -1,  7,  1],\n",
      "        [-1, -7,  1,  ..., -4, -8, -2]], dtype=torch.int8)... , scale=tensor([[0.0108, 0.0136, 0.0108,  ..., 0.0153, 0.0103, 0.0091],\n",
      "        [0.0143, 0.0141, 0.0114,  ..., 0.0083, 0.0135, 0.0183],\n",
      "        [0.0107, 0.0078, 0.0107,  ..., 0.0195, 0.0097, 0.0077],\n",
      "        ...,\n",
      "        [0.0177, 0.0142, 0.0139,  ..., 0.0116, 0.0142, 0.0220],\n",
      "        [0.0143, 0.0119, 0.0133,  ..., 0.0117, 0.0152, 0.0155],\n",
      "        [0.0121, 0.0106, 0.0142,  ..., 0.0130, 0.0164, 0.0130]])... , zero_point=tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])... , _layout=PlainLayout()), block_size=(1, 32), shape=torch.Size([768, 768]), device=cpu, dtype=torch.float32, requires_grad=False), <function _int8_asymm_per_token_quant at 0x7fc6bebe3380>, quant_kwargs={})), 'bias': tensor([ 5.4099e-01, -2.9702e-01, -4.0698e-01,  3.4577e-01, -2.9725e-01,\n",
      "         4.0322e-01,  1.7803e-02,  2.9310e-01,  2.4440e-01, -1.0622e-01,\n",
      "         1.2078e-01,  4.2102e-01, -9.2268e-02,  7.7929e-02,  4.2761e-01,\n",
      "         5.5778e-01,  2.1320e-02, -3.2888e-02, -3.0627e-01, -8.4873e-02,\n",
      "         2.1503e-02,  7.4768e-03,  1.4099e-02,  4.2945e-01, -8.5876e-03,\n",
      "        -4.4879e-02, -6.5838e-02,  5.6576e-01, -3.7549e-01, -9.8507e-02,\n",
      "         3.1143e-01,  2.1074e-02, -9.6579e-02, -2.3961e-01,  6.2665e-02,\n",
      "        -6.9571e-02, -6.9560e-02, -1.3181e-01,  5.1688e-01, -7.9177e-02,\n",
      "        -2.2471e-01, -3.8283e-01, -3.9461e-01, -1.7453e-01, -6.9795e-01,\n",
      "        -4.4101e-01, -1.3566e-01, -3.4118e-01,  4.4936e-02,  1.7475e-01,\n",
      "         4.1078e-01,  2.2792e-03,  6.5103e-01, -1.9682e-01,  2.9723e-01,\n",
      "        -2.8000e-01,  4.2087e-01,  8.1306e-02, -1.5777e-01, -2.4521e-01,\n",
      "         6.4294e-01,  7.2201e-02, -3.3086e-01,  1.5886e-01,  5.6324e-03,\n",
      "        -3.0930e-01,  5.1863e-02, -2.4012e-02,  2.1558e-01,  1.8015e-01,\n",
      "        -2.4142e-01, -3.6948e-01,  7.6507e-02,  1.8751e-01, -7.5772e-02,\n",
      "        -1.2791e-01, -1.8288e-01, -4.9914e-02,  1.0168e-01, -1.3798e-01,\n",
      "         4.2767e-02, -2.7149e-01, -1.2812e-03,  1.9407e-01, -2.3836e-01,\n",
      "         2.1527e-01,  1.6769e-01,  2.2770e-01, -5.8173e-02,  2.1725e-01,\n",
      "         3.4552e-01, -2.3567e-01, -1.4934e-01, -2.5601e-01,  1.1956e-01,\n",
      "         1.3635e-01, -5.4710e-02, -3.9618e-02, -2.0522e-01,  1.6081e-01,\n",
      "         5.4754e-02, -7.2280e-02,  2.6992e-01,  1.5360e-01,  9.1226e-01,\n",
      "        -2.4867e-01, -2.0115e-01,  1.3539e-01, -3.2857e-02, -1.0599e-01,\n",
      "        -2.8772e-02,  2.9510e-01,  1.6055e-01,  7.8075e-02,  9.2377e-02,\n",
      "        -7.5989e-01,  1.0122e-01,  7.7756e-02, -4.3827e-02, -2.6240e-01,\n",
      "         1.4493e-01, -3.8855e-02, -1.0964e-01, -9.1359e-02, -1.4609e-01,\n",
      "         8.3898e-02, -1.8679e-01,  1.9329e-01,  2.7447e-01,  9.9303e-01,\n",
      "        -6.9743e-01, -1.0970e-02,  2.9308e-02,  2.1718e-01, -1.5602e-01,\n",
      "         3.5761e-01, -2.1128e-01, -3.6380e-02, -2.8447e-01, -3.3133e-01,\n",
      "         2.6696e-01, -7.9780e-02,  3.5002e-01, -9.8447e-01, -1.7340e-01,\n",
      "         2.1975e-02,  2.8810e-01, -8.8832e-02, -1.1682e-01, -4.6230e-01,\n",
      "         3.3925e-01,  1.0289e+00,  3.4263e-01,  3.6416e-01, -6.9904e-01,\n",
      "         3.4575e-01,  3.8205e-01,  1.4424e-01, -2.2619e-01,  8.5852e-01,\n",
      "         7.3844e-01, -7.0926e-01,  5.1264e-01,  3.8644e-01, -7.7592e-01,\n",
      "         5.4812e-01, -2.5389e-02, -9.0862e-01, -2.8881e-01, -2.0915e-02,\n",
      "        -6.1836e-01, -8.6034e-01,  2.3824e-01,  4.7185e-01, -2.6671e-01,\n",
      "        -9.5471e-02, -9.5695e-01, -3.9383e-01, -1.5309e-01,  1.0338e+00,\n",
      "        -6.7643e-01,  7.5642e-01,  6.1964e-01,  8.2339e-02,  4.5345e-01,\n",
      "        -1.2759e-01, -8.3615e-01, -1.9943e-01,  7.5865e-01, -1.4626e-01,\n",
      "        -6.5414e-01, -3.8351e-01, -8.7606e-01,  7.5200e-02,  1.7769e-01,\n",
      "        -2.3490e-01,  1.1869e-01, -4.9632e-02,  5.2534e-01, -9.9556e-02,\n",
      "         2.0635e-02,  1.1973e-01,  5.4028e-02,  5.8047e-01, -6.6675e-02,\n",
      "        -3.8575e-01,  1.1892e-01, -1.8403e-01,  1.3351e-01,  2.1622e-01,\n",
      "         5.2837e-01,  5.5959e-02,  3.0311e-01, -4.4416e-01, -2.6638e-01,\n",
      "        -5.8798e-01, -5.9941e-01, -7.0103e-02,  8.3343e-01, -7.7126e-02,\n",
      "        -7.4098e-02,  3.5596e-01, -1.0781e+00,  2.1718e-01,  3.8459e-02,\n",
      "        -2.2325e-02,  2.0930e-01, -1.0136e-01,  1.1271e-01,  6.9503e-01,\n",
      "         6.7383e-01,  8.2713e-02, -1.9862e-01, -4.1268e-01, -2.6457e-01,\n",
      "        -2.0525e-01, -2.2675e-01,  3.6676e-01, -1.2882e-02,  6.1516e-02,\n",
      "        -4.5257e-01,  8.9481e-02, -1.4367e-01,  1.0353e-02, -1.4419e-01,\n",
      "        -1.7073e-01,  2.6768e-02,  6.2968e-02,  1.4330e-01,  2.4867e-01,\n",
      "        -7.6835e-01,  1.9503e-02, -7.7843e-03,  1.7836e-01, -1.3591e-01,\n",
      "        -1.9723e-01, -8.1304e-02,  4.2775e-02, -2.4652e-01, -1.3682e-02,\n",
      "        -1.0369e-01,  3.7988e-01,  3.1644e-01, -7.9138e-02,  4.1932e-02,\n",
      "        -2.0027e-01, -2.5067e-01,  7.7052e-02,  2.1090e-01,  2.9648e-01,\n",
      "        -2.8691e-02, -3.9233e-01, -2.1178e-02,  4.5173e-01, -4.2216e-02,\n",
      "         3.0370e-01,  6.8118e-02,  1.1424e-02, -8.8331e-02,  1.4208e-01,\n",
      "         3.0061e-01,  1.6471e-01,  3.4386e-01,  3.3691e-01, -5.1964e-01,\n",
      "         3.4416e-01, -9.0656e-02, -3.6553e-02, -3.3379e-02, -2.3693e-01,\n",
      "         1.8233e-01, -1.2251e-01,  1.1870e-01,  9.0419e-03, -3.5466e-01,\n",
      "         4.2403e-02, -4.9851e-01,  6.4359e-02,  2.0702e-02, -1.1249e-01,\n",
      "         4.7419e-01,  3.3812e-01, -2.6864e-01,  1.7600e-01, -2.1029e-01,\n",
      "        -2.2861e-01, -2.4042e-01,  1.8329e-01,  5.5931e-02, -2.4514e-01,\n",
      "        -1.1714e-01,  1.0413e-01, -2.1616e-01,  5.3320e-02,  4.6785e-01,\n",
      "         1.9049e-01, -1.1033e-01, -1.9086e-01,  1.2642e-01,  4.5217e-01,\n",
      "        -3.5459e-01, -2.9143e-01,  3.9219e-01, -2.4314e-01,  1.9765e-01,\n",
      "        -2.8363e-01, -1.5408e-01,  1.8223e-01,  2.0824e-01, -2.6249e-01,\n",
      "         1.1233e-01, -1.5735e-01,  3.0912e-01, -4.3544e-02,  9.6464e-02,\n",
      "         4.8163e-01,  3.5125e-02, -6.1810e-01,  3.3410e-02, -3.9146e-02,\n",
      "        -5.7276e-02,  4.1706e-01, -4.5236e-01, -1.7898e-01, -2.2765e-01,\n",
      "        -4.9262e-02, -4.8550e-01,  8.8568e-02,  1.4351e-01, -1.7119e-02,\n",
      "        -4.8013e-01,  4.0745e-02,  2.2576e-01, -3.3825e-01, -4.3848e-01,\n",
      "        -3.4731e-01, -6.3821e-01,  3.9908e-01,  1.5548e-01, -4.8308e-03,\n",
      "        -3.7991e-01,  2.2413e-01,  7.7405e-03, -3.6315e-01,  5.1618e-01,\n",
      "         1.2110e-01,  1.7492e-01,  3.9774e-01,  9.1610e-02, -1.2810e-01,\n",
      "        -2.2426e-01,  2.5368e-01,  2.7563e-01, -1.6866e-01, -3.1359e-01,\n",
      "         1.2796e-01, -2.9620e-01, -3.6397e-01, -1.2575e-01,  7.6573e-02,\n",
      "         5.5167e-02,  4.3302e-01, -1.6890e-01,  4.4844e-02,  2.3321e-01,\n",
      "         1.8324e-01,  7.1337e-02, -2.9953e-01, -5.5507e-01, -9.4501e-02,\n",
      "        -6.2411e-01,  2.4406e-02, -1.2369e-01, -3.7168e-01, -3.1348e-01,\n",
      "        -3.7436e-01, -6.3041e-01, -1.8939e-01, -2.3879e-01, -5.4687e-01,\n",
      "         6.4161e-01,  1.6757e-01,  1.9193e-01,  3.7058e-01,  1.0911e-01,\n",
      "         2.7146e-01, -1.9671e-01,  1.9835e-01, -2.5141e-01,  3.9253e-01,\n",
      "         8.4454e-02,  2.1524e-01, -2.7312e-01, -2.5204e-01, -1.8667e-02,\n",
      "        -5.1005e-01,  4.9059e-02,  2.8092e-01,  4.2794e-01,  4.2997e-02,\n",
      "        -7.5193e-02,  7.0251e-01, -3.3172e-01, -3.0988e-01, -5.6568e-02,\n",
      "         2.3964e-01, -2.7936e-01,  2.0491e-01, -3.2861e-02,  7.8771e-02,\n",
      "         9.6826e-02,  6.7528e-03,  6.9657e-02, -4.7105e-02, -6.4328e-03,\n",
      "        -1.6443e-02,  2.3242e-01, -1.3266e-01,  3.9143e-01,  2.8462e-01,\n",
      "        -4.7650e-01,  1.5829e-02,  1.5845e-01,  1.9194e-01, -4.4914e-01,\n",
      "        -3.4426e-01, -1.6152e-01,  5.4491e-01, -1.1194e-01, -9.0908e-02,\n",
      "         2.0713e-01,  1.2194e-01,  1.9682e-01,  5.5853e-02,  3.1585e-01,\n",
      "         1.2269e-01, -4.0473e-02,  1.9657e-02, -4.0232e-02, -1.5515e-01,\n",
      "        -7.5217e-02,  1.6501e-02,  5.7135e-02, -1.0494e-01, -3.0635e-01,\n",
      "         1.2533e-01, -7.4262e-02,  9.0495e-03, -2.1936e-01,  5.4930e-01,\n",
      "        -3.4551e-01, -2.0850e-01,  2.0752e-01, -1.4418e-01, -2.6096e-01,\n",
      "         1.4803e-01,  1.7043e-01,  3.4334e-01, -4.3233e-01, -4.2570e-01,\n",
      "         3.7528e-02,  1.8925e-01, -2.2382e-02, -3.6336e-01, -4.0099e-01,\n",
      "         1.6628e-01,  1.2134e-01,  7.8744e-02,  1.8656e-01, -3.4125e-02,\n",
      "        -1.3603e-01, -8.4014e-02,  3.8934e-01, -2.6855e-02, -7.8231e-02,\n",
      "        -8.9778e-02,  9.7950e-02, -1.1016e-01,  2.6389e-01, -1.0901e-01,\n",
      "         1.6954e-01,  3.7727e-01, -4.0571e-01, -1.2351e-01, -3.0096e-02,\n",
      "        -9.4989e-02,  4.8798e-03, -2.6732e-01, -1.5729e-02, -1.3319e-01,\n",
      "         1.2588e-01,  8.1980e-02, -4.2521e-02,  2.7019e-01,  1.4131e-01,\n",
      "         3.4718e-01, -1.4010e-01, -2.4171e-01, -1.4709e-01,  6.5403e-02,\n",
      "         1.6920e-01,  1.2378e-01,  1.8495e-01,  1.6922e-01, -9.1019e-02,\n",
      "         1.4013e-01, -1.1091e-01,  4.8958e-01,  1.1617e-01,  6.3151e-01,\n",
      "        -1.5084e-01, -2.1589e-01,  3.2275e-01, -1.7839e-01, -9.5719e-02,\n",
      "         1.4630e-01, -5.5553e-01, -5.2821e-01, -2.4663e-01, -1.7115e-01,\n",
      "        -1.1746e-01,  4.0441e-01, -2.4109e-01,  5.9325e-01, -5.4541e-02,\n",
      "         2.7731e-02,  2.9772e-01,  2.5456e-01,  1.2206e-01,  2.2988e-01,\n",
      "        -5.6732e-02, -4.9993e-02,  2.8720e-02,  2.3265e-01, -2.4644e-01,\n",
      "         3.0752e-01,  1.7946e-01, -1.6740e-01, -2.9588e-02, -3.5777e-01,\n",
      "        -4.2494e-01,  2.6060e-01, -1.1927e-01,  6.9076e-01,  2.8038e-01,\n",
      "         4.9503e-01, -2.1291e-01, -4.0916e-01, -3.0502e-01, -2.8854e-01,\n",
      "         9.9206e-02,  7.0715e-02, -7.9820e-02, -3.5917e-03, -2.8431e-01,\n",
      "         8.5828e-01, -4.2529e-01, -3.0067e-01, -6.3072e-04, -1.5492e-01,\n",
      "         3.9782e-01,  2.4771e-01,  1.0721e-01,  1.3407e-01, -2.3667e-01,\n",
      "        -6.3285e-01,  1.4746e-01,  2.1490e-01, -3.1387e-01,  3.5973e-01,\n",
      "        -1.9057e-02, -2.8147e-01,  1.8794e-01, -2.4442e-01, -1.9665e-01,\n",
      "         1.8234e-01,  6.2215e-01, -1.0537e-01,  8.7471e-03, -5.1878e-01,\n",
      "         7.0999e-01,  5.1980e-01, -1.7690e-01,  2.6034e-01, -2.7744e-01,\n",
      "        -2.6919e-02,  6.5017e-01, -4.0419e-02, -5.7865e-02, -7.0137e-03,\n",
      "         3.0307e-01, -1.9089e-01, -3.8719e-02,  3.9394e-01, -5.2333e-01,\n",
      "        -3.0450e-01,  3.8790e-01,  2.1359e-01, -1.6310e-01, -2.4644e-02,\n",
      "         5.2482e-01, -3.7244e-01, -4.2175e-01,  6.5197e-02, -2.7442e-01,\n",
      "        -4.9337e-01,  2.9549e-01,  3.5771e-01, -5.8860e-01,  1.8514e-01,\n",
      "         5.8188e-02, -1.8675e-01,  6.2209e-01, -2.7107e-01, -3.6132e-01,\n",
      "         2.4930e-01,  4.6533e-01,  4.9400e-01, -2.0610e-01, -4.4744e-01,\n",
      "        -3.1825e-01,  3.8156e-01,  2.3815e-01,  3.7604e-01,  1.0231e+00,\n",
      "        -4.4387e-01,  4.0833e-01,  7.7900e-01,  3.5716e-01,  4.0623e-01,\n",
      "         1.3448e-01,  1.8734e-01, -8.0652e-01, -2.8810e-01,  1.8338e-01,\n",
      "        -3.8372e-01,  7.4401e-01,  6.4708e-02,  8.8733e-02,  1.7781e-01,\n",
      "        -1.8539e-01,  1.1963e-01, -5.7390e-01,  1.8608e-01,  1.1005e+00,\n",
      "        -1.8635e-02, -6.4379e-02,  3.1987e-01, -1.9159e-01,  3.9563e-01,\n",
      "         4.5455e-02,  1.3070e-01,  8.3459e-01,  1.1356e-01, -1.0572e-01,\n",
      "         3.4327e-01,  3.8593e-01, -3.3498e-01, -7.0198e-01,  6.0231e-01,\n",
      "         6.1608e-01,  7.2436e-02, -5.2502e-01,  3.8967e-02, -2.9245e-01,\n",
      "         1.7584e-01,  2.1646e-01,  8.4276e-01,  3.2985e-01, -4.8586e-01,\n",
      "         2.8991e-01,  5.8266e-02,  5.6988e-02,  9.9912e-02, -3.2691e-01,\n",
      "         2.4261e-01, -8.9265e-02, -8.3037e-01,  3.1262e-02, -6.1935e-02,\n",
      "        -6.2905e-01,  6.1448e-01, -1.3347e-01,  6.1071e-01,  1.9603e-01,\n",
      "         3.3796e-01, -1.2955e-01, -3.8936e-01, -1.0098e-02, -1.5622e-01,\n",
      "        -1.6747e-01,  4.6662e-01, -3.2092e-01, -4.1947e-02, -1.4595e-01,\n",
      "        -4.1272e-01,  4.2343e-01,  2.3737e-01,  4.7531e-03, -1.1805e-01,\n",
      "         1.7282e-01, -2.6049e-01, -5.8748e-02,  4.1173e-01,  4.4212e-01,\n",
      "        -9.1442e-02,  5.3569e-02, -6.8478e-02, -2.1597e-01, -3.7656e-01,\n",
      "         9.3200e-02, -8.4359e-02,  4.6635e-01, -3.1796e-01,  3.4587e-01,\n",
      "        -6.2787e-01,  1.2872e-01, -1.2155e-02, -1.8622e-01, -6.7626e-02,\n",
      "        -2.8658e-01, -3.2254e-01, -4.7035e-01,  6.4083e-02, -2.1320e-01,\n",
      "         2.4955e-01,  5.5013e-01, -9.0951e-02,  2.7925e-01,  2.8434e-01,\n",
      "         2.8066e-01,  1.5929e-01, -2.0303e-03, -1.9519e-01,  5.6878e-02,\n",
      "         1.6365e-01,  4.2419e-03, -2.0773e-01, -1.3231e-01, -1.1365e-02,\n",
      "         2.8529e-01,  8.1127e-02,  3.7672e-01,  4.0929e-02, -2.6936e-01,\n",
      "         2.7614e-01, -1.2889e-01, -1.4062e-02])})\n"
     ]
    }
   ],
   "source": [
    "print(model.distilbert.transformer.layer[0].attention.q_lin.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(tokens[\"train\"][\"input_ids\"][0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.0180, grad_fn=<NllLossBackward0>), logits=tensor([[ 1.8608, -1.8304],\n",
      "        [ 2.2928, -2.0456],\n",
      "        [ 2.1260, -2.0180],\n",
      "        [ 2.0767, -1.9183],\n",
      "        [ 2.1248, -2.0246],\n",
      "        [ 2.0277, -1.8520],\n",
      "        [ 2.0801, -2.0569],\n",
      "        [ 2.0196, -1.8730]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    print(model(**batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[32m      2\u001b[39m     batch = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch.items()}\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mqmodel\u001b[49m(**batch))\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'qmodel' is not defined"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    print(qmodel(**batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"acc after freeze:\", trainer.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantize(model)\n",
    "trainer.train()\n",
    "freeze(model)\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
