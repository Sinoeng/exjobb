{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import DistilBertModel, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from transformers.models.distilbert import modeling_distilbert\n",
    "import datasets\n",
    "from transformers import TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from typing import Dict, List, Optional, Set, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iMultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.dim = config.dim\n",
    "        self.dropout = nn.Dropout(p=config.attention_dropout)\n",
    "        self.is_causal = False\n",
    "\n",
    "        # Have an even number of multi heads that divide the dimensions\n",
    "        if self.dim % self.n_heads != 0:\n",
    "            # Raise value errors for even multi-head attention nodes\n",
    "            raise ValueError(f\"self.n_heads: {self.n_heads} must divide self.dim: {self.dim} evenly\")\n",
    "\n",
    "        self.q_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.k_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.v_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.out_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "\n",
    "        self.gamma = 1\n",
    "        self.delta = 1\n",
    "        self.eta = 1\n",
    "\n",
    "        self.pruned_heads: Set[int] = set()\n",
    "        self.attention_head_size = self.dim // self.n_heads\n",
    "\n",
    "    def prune_heads(self, heads: List[int]):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.n_heads, self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "        # Prune linear layers\n",
    "        self.q_lin = prune_linear_layer(self.q_lin, index)\n",
    "        self.k_lin = prune_linear_layer(self.k_lin, index)\n",
    "        self.v_lin = prune_linear_layer(self.v_lin, index)\n",
    "        self.out_lin = prune_linear_layer(self.out_lin, index, dim=1)\n",
    "        # Update hyper params\n",
    "        self.n_heads = self.n_heads - len(heads)\n",
    "        self.dim = self.attention_head_size * self.n_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def compute_relu_attention(self, scores: torch.Tensor, mask: torch.Tensor, \n",
    "                               head_mask: Optional[torch.Tensor], v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        scores = F.relu(scores)\n",
    "        weights = scores\n",
    "        M = torch.max(torch.maximum(v, -v)) + 1\n",
    "        mask = self.dropout(mask.to(scores.dtype))\n",
    "        mask = (mask == 0).unsqueeze(1).unsqueeze(2)\n",
    "        scores = scores.masked_fill(mask, M)\n",
    "        v_t = v.transpose(-1, -2)\n",
    "        \n",
    "        if head_mask is not None:\n",
    "            weights = weights * head_mask\n",
    "        \n",
    "        context = self.compute_signed_inhibitor(scores, v, v_t)\n",
    "\n",
    "        return context, weights\n",
    "    \n",
    "    def compute_signed_inhibitor(self, scores: torch.Tensor, v: torch.Tensor, v_t: torch.Tensor) -> torch.Tensor:\n",
    "        pos_v = F.relu(v_t)\n",
    "        neg_v = -F.relu(-v_t)\n",
    "        v_sum = torch.sum(v, dim=-2, keepdim=True)\n",
    "        dist1 = torch.cdist(scores, pos_v, p=1)\n",
    "        dist2 = torch.cdist(scores, -neg_v, p=1)\n",
    "        return 0.5 * (v_sum + dist1 - dist2)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            query: torch.tensor(bs, seq_length, dim)\n",
    "            key: torch.tensor(bs, seq_length, dim)\n",
    "            value: torch.tensor(bs, seq_length, dim)\n",
    "            mask: torch.tensor(bs, seq_length)\n",
    "\n",
    "        Returns:\n",
    "            weights: torch.tensor(bs, n_heads, seq_length, seq_length) Attention weights context: torch.tensor(bs,\n",
    "            seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\n",
    "        \"\"\"\n",
    "        bs, q_length, dim = query.size()\n",
    "        k_length = key.size(1)\n",
    "        # assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'\n",
    "        # assert key.size() == value.size()\n",
    "\n",
    "        dim_per_head = self.dim // self.n_heads\n",
    "\n",
    "        mask_reshp = (bs, 1, 1, k_length)\n",
    "\n",
    "        def shape(x: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"separate heads\"\"\"\n",
    "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
    "\n",
    "        def unshape(x: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"group heads\"\"\"\n",
    "            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
    "\n",
    "        q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)\n",
    "        k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)\n",
    "        v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_per_head)\n",
    "\n",
    "        #q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        #scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\n",
    "        scale = self.gamma/math.sqrt(dim_per_head)\n",
    "        scores = torch.cdist(q, k, p=1) * scale # Z\n",
    "\n",
    "        mean = torch.mean(scores, dim = -1, keepdim = True)\n",
    "        scores -= mean\n",
    "            \n",
    "        #shift score\n",
    "        if self.alpha > 0:\n",
    "            scores += self.alpha  \n",
    "\n",
    "        context, weights = self.compute_relu_attention(scores, mask, head_mask, v)\n",
    "        context *= self.beta #scale context\n",
    "\n",
    "        context = unshape(context)  # (bs, q_length, dim)\n",
    "        context = self.out_lin(context)  # (bs, q_length, dim)\n",
    "\n",
    "        if output_attentions:\n",
    "            return (context, weights)\n",
    "        else:\n",
    "            return (context,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_distilbert.MultiHeadSelfAttention = iMultiHeadSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"imdb\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    tokens = tokenizer(data[\"text\"], truncation=True, padding = 'max_length',  max_length=512)\n",
    "    tokens[\"label\"] = data[\"label\"]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [01:31<00:00, 274.53 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [01:24<00:00, 297.43 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [03:02<00:00, 274.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokens = dataset.map(preprocess, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokens['train'].features['label'].names\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = {}, {}\n",
    "\n",
    "for idx, lbl in enumerate(labels):\n",
    "    label2id[lbl] = idx\n",
    "    id2label[idx] = lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokens[\"train\"].shuffle(seed=11).select(range(2000))\n",
    "small_eval_dataset = tokens[\"train\"].shuffle(seed=11).select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DistilBertForSequenceClassification, AutoModelForSequenceClassification, DistilBertConfig, DataCollatorWithPadding\n\u001b[32m      3\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mmodel_id\u001b[49m,\n\u001b[32m      5\u001b[39m     num_labels = num_labels,\n\u001b[32m      6\u001b[39m     id2label = id2label,\n\u001b[32m      7\u001b[39m     label2id = label2id,\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'model_id' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AutoModelForSequenceClassification, DistilBertConfig, DataCollatorWithPadding\n",
    "model_id = \"distilbert-base-uncased\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels = num_labels,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Preprocess the logits to ensure they are in the correct format for metric computation.\n",
    "    This function will be called during the evaluation process.\n",
    "    \"\"\"\n",
    "    if isinstance(logits, tuple):  \n",
    "        logits = logits[0]  # get logit tensors\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    return pred_ids, labels\n",
    "    \n",
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    return accuracy.compute(predictions=predictions[0], references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.00005\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './imdb_tune_distilbert',\n",
    "    num_train_epochs = EPOCHS,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    logging_dir = './logs',\n",
    "    load_best_model_at_end= True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps = 500,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=['tensorboard'],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=small_train_dataset,         \n",
    "    eval_dataset=small_eval_dataset.shuffle(seed=72).select(range(600)),\n",
    "    compute_metrics = compute_metrics,\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
