{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/coding/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-10 16:24:58.819062: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741620298.883636   16460 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741620298.901421   16460 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-10 16:24:59.033660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import DistilBertModel, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from transformers.models.distilbert import modeling_distilbert\n",
    "import datasets\n",
    "from transformers import TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iMultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.dim = config.dim\n",
    "        self.dropout = nn.Dropout(p=config.attention_dropout)\n",
    "        self.is_causal = False\n",
    "\n",
    "        # Have an even number of multi heads that divide the dimensions\n",
    "        if self.dim % self.n_heads != 0:\n",
    "            # Raise value errors for even multi-head attention nodes\n",
    "            raise ValueError(f\"self.n_heads: {self.n_heads} must divide self.dim: {self.dim} evenly\")\n",
    "\n",
    "        self.q_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.k_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.v_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.out_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "\n",
    "        self.gamma = 1\n",
    "        self.delta = 1\n",
    "        self.eta = 1\n",
    "\n",
    "        self.pruned_heads: Set[int] = set()\n",
    "        self.attention_head_size = self.dim // self.n_heads\n",
    "\n",
    "    def prune_heads(self, heads: List[int]):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.n_heads, self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "        # Prune linear layers\n",
    "        self.q_lin = prune_linear_layer(self.q_lin, index)\n",
    "        self.k_lin = prune_linear_layer(self.k_lin, index)\n",
    "        self.v_lin = prune_linear_layer(self.v_lin, index)\n",
    "        self.out_lin = prune_linear_layer(self.out_lin, index, dim=1)\n",
    "        # Update hyper params\n",
    "        self.n_heads = self.n_heads - len(heads)\n",
    "        self.dim = self.attention_head_size * self.n_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def compute_relu_attention(self, scores: torch.Tensor, mask: torch.Tensor, \n",
    "                               head_mask: Optional[torch.Tensor], v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        scores = F.relu(scores)\n",
    "        weights = scores\n",
    "        M = torch.max(torch.maximum(v, -v)) + 1\n",
    "        mask = self.dropout(mask.to(scores.dtype))\n",
    "        mask = (mask == 0).unsqueeze(1).unsqueeze(2)\n",
    "        scores = scores.masked_fill(mask, M)\n",
    "        v_t = v.transpose(-1, -2)\n",
    "        \n",
    "        if head_mask is not None:\n",
    "            weights = weights * head_mask\n",
    "        \n",
    "        context = self.compute_signed_inhibitor(scores, v, v_t)\n",
    "\n",
    "        return context, weights\n",
    "    \n",
    "    def compute_signed_inhibitor(self, scores: torch.Tensor, v: torch.Tensor, v_t: torch.Tensor) -> torch.Tensor:\n",
    "        pos_v = F.relu(v_t)\n",
    "        neg_v = -F.relu(-v_t)\n",
    "        v_sum = torch.sum(v, dim=-2, keepdim=True)\n",
    "        dist1 = torch.cdist(scores, pos_v, p=1)\n",
    "        dist2 = torch.cdist(scores, -neg_v, p=1)\n",
    "        return 0.5 * (v_sum + dist1 - dist2)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            query: torch.tensor(bs, seq_length, dim)\n",
    "            key: torch.tensor(bs, seq_length, dim)\n",
    "            value: torch.tensor(bs, seq_length, dim)\n",
    "            mask: torch.tensor(bs, seq_length)\n",
    "\n",
    "        Returns:\n",
    "            weights: torch.tensor(bs, n_heads, seq_length, seq_length) Attention weights context: torch.tensor(bs,\n",
    "            seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\n",
    "        \"\"\"\n",
    "        bs, q_length, dim = query.size()\n",
    "        k_length = key.size(1)\n",
    "        # assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'\n",
    "        # assert key.size() == value.size()\n",
    "\n",
    "        dim_per_head = self.dim // self.n_heads\n",
    "\n",
    "        mask_reshp = (bs, 1, 1, k_length)\n",
    "\n",
    "        def shape(x: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"separate heads\"\"\"\n",
    "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
    "\n",
    "        def unshape(x: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"group heads\"\"\"\n",
    "            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
    "\n",
    "        q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)\n",
    "        k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)\n",
    "        v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_per_head)\n",
    "\n",
    "        #q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        #scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\n",
    "        scale = self.gamma/math.sqrt(dim_per_head)\n",
    "        scores = torch.cdist(q, k, p=1) * scale # Z\n",
    "\n",
    "        mean = torch.mean(scores, dim = -1, keepdim = True)\n",
    "        scores -= mean\n",
    "            \n",
    "        #shift score\n",
    "        if self.alpha > 0:\n",
    "            scores += self.alpha  \n",
    "\n",
    "        context, weights = self.compute_relu_attention(scores, mask, head_mask, v)\n",
    "        context *= self.beta #scale context\n",
    "\n",
    "        context = unshape(context)  # (bs, q_length, dim)\n",
    "        context = self.out_lin(context)  # (bs, q_length, dim)\n",
    "\n",
    "        if output_attentions:\n",
    "            return (context, weights)\n",
    "        else:\n",
    "            return (context,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_distilbert.MultiHeadSelfAttention = iMultiHeadSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pipeline(task=\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "p(\"I like eating [MASK]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
